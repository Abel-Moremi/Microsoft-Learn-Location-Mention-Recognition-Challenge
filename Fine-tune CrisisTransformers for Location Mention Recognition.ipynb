{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check working dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abelm\\OneDrive\\Documents\\GitHub\\Microsoft-Learn-Location-Mention-Recognition-Challenge\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "#os.chdir('path_to_directory_where_file_is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 260: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m                 sentence, label \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentences, labels\n\u001b[1;32m---> 16\u001b[0m train_sentences, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mread_bioes_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/kaggle/merged_train_bioes.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m dev_sentences, dev_labels \u001b[38;5;241m=\u001b[39m read_bioes_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/kaggle/merged_dev_bioes.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m, in \u001b[0;36mread_bioes_file\u001b[1;34m(file_path, encoding)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      4\u001b[0m     sentence, label \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[0;32m      7\u001b[0m             word, tag \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mllmrc\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 260: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "def read_bioes_file(file_path):\n",
    "    sentences, labels = [], []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            sentence, label = [], []\n",
    "            for line in file:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        word, tag = line.strip().split()\n",
    "                        sentence.append(word)\n",
    "                        label.append(tag)\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping malformed line: {line.strip()}\")\n",
    "                else:\n",
    "                    if sentence and label:\n",
    "                        sentences.append(sentence)\n",
    "                        labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "        return sentences, labels\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Unicode decoding error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        \n",
    "train_sentences, train_labels = read_bioes_file('data/kaggle/merged_train_bioes.txt')\n",
    "dev_sentences, dev_labels = read_bioes_file('data/kaggle/merged_dev_bioes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train sentences: \", len(train_sentences))\n",
    "print(\"Train labels: \", len(train_labels))\n",
    "print(\"Dev sentences: \", len(dev_sentences))\n",
    "print(\"Dev labels: \", len(dev_labels))\n",
    "print()\n",
    "\n",
    "print(\"Train sentences: \\n\", train_sentences[0])\n",
    "print(\"Train labels: \\n\", train_labels[0])\n",
    "print(\"Dev sentences: \\n\", dev_sentences[0])\n",
    "print(\"Dev labels: \\n\", dev_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up label mapping\n",
    "all_labels = set()\n",
    "\n",
    "for labels in train_labels + dev_labels:\n",
    "    all_labels.update(labels)\n",
    "\n",
    "label_list = sorted(list(all_labels))\n",
    "\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train_labels to IDs\n",
    "train_labels_ids = [[label2id[label] for label in sentence_labels] for sentence_labels in train_labels]\n",
    "dev_labels_ids = [[label2id[label] for label in sentence_labels] for sentence_labels in dev_labels]\n",
    "\n",
    "# Example usage\n",
    "print(\"Original first sentence labels:\", train_labels[0])\n",
    "print(\"Converted first sentence label IDs:\", train_labels_ids[0])\n",
    "\n",
    "print(\"Valid length: \", len(train_labels) == len(train_labels_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sentence and label from BIOES to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def preprocess_sentence_and_labels(sentence, labels):\n",
    "    processed_sentence = []\n",
    "    processed_labels = []\n",
    "\n",
    "    for word, label in zip(sentence, labels):\n",
    "        # Remove words with special characters or numbers\n",
    "        if not re.match(r'^[a-zA-Z]+$', word):\n",
    "            continue\n",
    "\n",
    "        # If the word is not empty after processing, keep it and its label\n",
    "        if word:\n",
    "            processed_sentence.append(word)\n",
    "            processed_labels.append(label)\n",
    "\n",
    "    return processed_sentence, processed_labels\n",
    "\n",
    "# Process the training data\n",
    "processed_train_sentences = []\n",
    "processed_train_labels_ids = []\n",
    "\n",
    "# Process the training data\n",
    "processed_dev_sentences = []\n",
    "processed_dev_labels_ids = []\n",
    "\n",
    "for sentence, labels in zip(train_sentences, train_labels_ids):\n",
    "    proc_sentence, proc_labels = preprocess_sentence_and_labels(sentence, labels)\n",
    "    processed_train_sentences.append(proc_sentence)\n",
    "    processed_train_labels_ids.append(proc_labels)\n",
    "\n",
    "for sentence, labels in zip(dev_sentences, dev_labels_ids):\n",
    "    proc_sentence, proc_labels = preprocess_sentence_and_labels(sentence, labels)\n",
    "    processed_dev_sentences.append(proc_sentence)\n",
    "    processed_dev_labels_ids.append(proc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print an example to compare\n",
    "print(\"Original sentence:\", train_sentences[2])\n",
    "print(\"Original labels:\", train_labels_ids[2])\n",
    "print(\"\\nProcessed sentence:\", processed_train_sentences[2])\n",
    "print(len(processed_train_sentences[2]))\n",
    "print(\"Processed labels:\", processed_train_labels_ids[2])\n",
    "print(len(processed_train_labels_ids[2]))\n",
    "\n",
    "# Print some statistics\n",
    "original_word_count = sum(len(sentence) for sentence in train_sentences)\n",
    "processed_word_count = sum(len(sentence) for sentence in processed_train_sentences)\n",
    "print(f\"\\nOriginal word count: {original_word_count}\")\n",
    "print(f\"Processed word count: {processed_word_count}\")\n",
    "print(f\"Removed {original_word_count - processed_word_count} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "CT_M3_Complete_tokenizer = AutoTokenizer.from_pretrained(\"crisistransformers/CT-M3-Complete\")\n",
    "\n",
    "def tokenize_and_adjust_labels(sentence: List[str], labels: List[int], tokenizer) -> Tuple[List[int], List[int]]:\n",
    "    tokenized_input = tokenizer(sentence, is_split_into_words=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "\n",
    "    updated_labels = []\n",
    "    current_label_idx = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in ['<s>', '</s>', '<unk>']:\n",
    "            updated_labels.append(-100)\n",
    "        elif token.endswith('@@'):  # Handle word pieces ending with @@\n",
    "            updated_labels.append(labels[current_label_idx])\n",
    "        else:\n",
    "            updated_labels.append(labels[current_label_idx])\n",
    "            current_label_idx += 1\n",
    "\n",
    "    return tokenized_input[\"input_ids\"], updated_labels\n",
    "\n",
    "# Apply the function to all sentences and labels\n",
    "tokenized_train_inputs = []\n",
    "adjusted_train_labels = []\n",
    "\n",
    "tokenized_dev_inputs = []\n",
    "adjusted_dev_labels = []\n",
    "\n",
    "for sentence, labels in zip(processed_train_sentences, processed_train_labels_ids):\n",
    "    input_ids, adjusted_labels = tokenize_and_adjust_labels(sentence, labels, CT_M3_Complete_tokenizer)\n",
    "    tokenized_train_inputs.append(input_ids)\n",
    "    adjusted_train_labels.append(adjusted_labels)\n",
    "\n",
    "for sentence, labels in zip(processed_dev_sentences, processed_dev_labels_ids):\n",
    "    input_ids, adjusted_labels = tokenize_and_adjust_labels(sentence, labels, CT_M3_Complete_tokenizer)\n",
    "    tokenized_dev_inputs.append(input_ids)\n",
    "    adjusted_dev_labels.append(adjusted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print an example to verify\n",
    "print(\"Original sentence:\", processed_train_sentences[2])\n",
    "print(\"Original labels:\", processed_train_labels_ids[2])\n",
    "print(\"\\nTokenized input:\", tokenized_train_inputs[2])\n",
    "print(\"Adjusted labels:\", adjusted_train_labels[2])\n",
    "​\n",
    "# Verify lengths\n",
    "print(\"\\nLength of tokenized input:\", len(tokenized_train_inputs[2]))\n",
    "print(\"Length of adjusted labels:\", len(adjusted_train_labels[2]))\n",
    "​\n",
    "# Print some statistics\n",
    "original_sentence_count = len(input_ids)\n",
    "tokenized_sentence_count = len(tokenized_train_inputs)\n",
    "print(f\"\\nNumber of original sentences: {original_sentence_count}\")\n",
    "print(f\"Number of tokenized sentences: {tokenized_sentence_count}\")\n",
    "​\n",
    "average_original_length = sum(len(s) for s in processed_train_sentences) / original_sentence_count\n",
    "average_tokenized_length = sum(len(s) for s in tokenized_train_inputs) / tokenized_sentence_count\n",
    "print(f\"\\nAverage original sentence length: {average_original_length:.2f}\")\n",
    "print(f\"Average tokenized sentence length: {average_tokenized_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert to datasets\n",
    "tokenized_train = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_train_inputs,\n",
    "    \"labels\": adjusted_train_labels\n",
    "})\n",
    "tokenized_dev = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_dev_inputs,\n",
    "    \"labels\": adjusted_dev_labels\n",
    "})\n",
    "\n",
    "# Set up label mapping\n",
    "all_labels = set()\n",
    "\n",
    "for labels in train_labels + dev_labels:\n",
    "    all_labels.update(labels)\n",
    "\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "labels = sorted(list(all_labels))\n",
    "\n",
    "label_list = sorted(list(all_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification, AutoModelForMaskedLM\n",
    "\n",
    "model_name = \"crisistransformers/CT-M3-Complete\"\n",
    "\n",
    "# Update model configuration\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = len(label_list)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "\n",
    "CT_M3_Complete_model = AutoModelForTokenClassification.from_pretrained(model_name, config=config)\n",
    "CT_M3_Complete_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = precision_recall_fscore_support(sum(true_labels, []), sum(true_predictions, []), average='weighted')\n",
    "    return {\n",
    "        \"precision\": results[0],\n",
    "        \"recall\": results[1],\n",
    "        \"f1\": results[2],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up model and tokenizer\n",
    "model_name = \"crisistransformers/CT-M3-Complete\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = len(label_list)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=CT_M3_Complete_tokenizer)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"content/drive/MyDrive/CrisisTransformers\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Reduced batch size\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"adamw_torch\",  # Use PyTorch's AdamW implementation\n",
    "    logging_steps=100,  # Reduce logging frequency\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "    report_to='none',  # Disable logging to wandb\n",
    ")\n",
    "\n",
    "# Set up trainer\n",
    "trainer = Trainer(\n",
    "    model=CT_M3_Complete_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    tokenizer=CT_M3_Complete_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# After training\n",
    "output_dir = \"/kaggle/working/results\"\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "CT_M3_Complete_tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save training arguments\n",
    "with open(f\"{output_dir}/training_args.json\", 'w') as f:\n",
    "    json.dump(training_args.to_dict(), f)\n",
    "\n",
    "# Save label mappings\n",
    "with open(f\"{output_dir}/label_mappings.json\", 'w') as f:\n",
    "    json.dump({\"label2id\": label2id, \"id2label\": id2label}, f)\n",
    "\n",
    "print(f\"Model and associated files saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"/kaggle/working/results\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/results\")\n",
    "\n",
    "# Load label mappings\n",
    "with open(\"/kaggle/working/results/label_mappings.json\", 'r') as f:\n",
    "    label_mappings = json.load(f)\n",
    "\n",
    "id2label = label_mappings[\"id2label\"]\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_subwords_and_locations(tokens_and_labels):\n",
    "    merged_words = []\n",
    "    merged_labels = []\n",
    "    current_word = []\n",
    "    current_labels = []\n",
    "    location_buffer = []\n",
    "\n",
    "    for token, label in tokens_and_labels:\n",
    "        if token.endswith('@@'):\n",
    "            current_word.append(token[:-2])  # Remove '@@'\n",
    "            current_labels.append(label)\n",
    "        else:\n",
    "            current_word.append(token)\n",
    "            current_labels.append(label)\n",
    "\n",
    "            # Merge subwords\n",
    "            merged_word = ''.join(current_word)\n",
    "\n",
    "            # Voting for the label\n",
    "            if len(set(current_labels)) == 1:\n",
    "                merged_label = current_labels[0]\n",
    "            else:\n",
    "                priority_order = ['B-LOC', 'I-LOC', 'E-LOC', 'S-LOC', 'O']\n",
    "                merged_label = next(label for label in priority_order if label in current_labels)\n",
    "\n",
    "            # Handle location merging\n",
    "            if merged_label.endswith('-LOC'):\n",
    "                if merged_label == 'B-LOC' or merged_label == 'S-LOC':\n",
    "                    if location_buffer:\n",
    "                        merged_words.append(' '.join(location_buffer))\n",
    "                        merged_labels.append('B-LOC')\n",
    "                        location_buffer = []\n",
    "                    location_buffer.append(merged_word)\n",
    "                elif merged_label == 'I-LOC' or merged_label == 'E-LOC':\n",
    "                    location_buffer.append(merged_word)\n",
    "                    if merged_label == 'E-LOC':\n",
    "                        merged_words.append(' '.join(location_buffer))\n",
    "                        merged_labels.append('B-LOC')\n",
    "                        location_buffer = []\n",
    "            else:\n",
    "                if location_buffer:\n",
    "                    merged_words.append(' '.join(location_buffer))\n",
    "                    merged_labels.append('B-LOC')\n",
    "                    location_buffer = []\n",
    "                merged_words.append(merged_word)\n",
    "                merged_labels.append(merged_label)\n",
    "\n",
    "            # Reset for next word\n",
    "            current_word = []\n",
    "            current_labels = []\n",
    "\n",
    "    # Handle any remaining location in the buffer\n",
    "    if location_buffer:\n",
    "        merged_words.append(' '.join(location_buffer))\n",
    "        merged_labels.append('B-LOC')\n",
    "\n",
    "    return list(zip(merged_words, merged_labels))\n",
    "\n",
    "# # Usage\n",
    "# merged_result = merge_subwords_and_locations(predicted_tokens)\n",
    "\n",
    "# # Extract locations\n",
    "# locations = [word for word, label in merged_result if label == 'B-LOC']\n",
    "# print(\"\\nExtracted locations:\", locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
    "\n",
    "    tokens = []\n",
    "    predicted_tokens = []\n",
    "\n",
    "    locations = []\n",
    "    current_location = []\n",
    "\n",
    "    for token, prediction in zip(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]), predictions[0]):\n",
    "        if int(prediction) == 0:  # Beginning of a new location\n",
    "            current_location = [token]\n",
    "        elif int(prediction) == 2:  # Inside a location\n",
    "            if current_location:  # Make sure we started a location\n",
    "                current_location.append(token)\n",
    "        elif int(prediction) == 1:  # End of a location\n",
    "            if current_location:  # Make sure we're inside a location\n",
    "                current_location.append(token)\n",
    "                locations.append(\" \".join(current_location))\n",
    "                current_location = []\n",
    "        elif int(prediction) == 4:  # Single token location\n",
    "            locations.append(token)\n",
    "        else:\n",
    "            current_location = []  # Reset if prediction is 'O' or anything else\n",
    "\n",
    "        # Remove special tokens and clean up the text\n",
    "        if token not in ['<s>', '</s>', '<unk>']:\n",
    "            cleaned_token = token[1:] if token.startswith('Ġ') else token\n",
    "\n",
    "            if token.startswith('##'):\n",
    "                if predicted_tokens:\n",
    "                    predicted_tokens[-1] = (predicted_tokens[-1][0] + cleaned_token, predicted_tokens[-1][1])\n",
    "                continue\n",
    "\n",
    "            tokens.append(cleaned_token)\n",
    "            predicted_tokens.append((cleaned_token, id2label[str(prediction.item())]))\n",
    "\n",
    "    # Usage\n",
    "    merged_result = merge_subwords_and_locations(predicted_tokens)\n",
    "\n",
    "    # Extract locations\n",
    "    locations = [word for word, label in merged_result if label == 'B-LOC']\n",
    "\n",
    "    # Extract unique locations and sort alphabetically\n",
    "    unique_locations = sorted(set(locations))\n",
    "\n",
    "    return unique_locations, tokens, predictions, predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "test = pd.read_csv(\"/kaggle/input/zindi-learn-location-mention-recognition-challenge/Test.csv\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import nltk\n",
    "nltk.data.path.append('/usr/share/nltk_data/')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '<URL>', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\./\\-_]', '', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "#     # Lemmatize\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into a string\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Apply preprocessing to each text in your dataset\n",
    "test['processed_text'] = test['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 /kaggle/submission.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
