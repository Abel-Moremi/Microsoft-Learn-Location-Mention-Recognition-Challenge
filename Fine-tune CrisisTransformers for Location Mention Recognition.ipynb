{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS AND CHECKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abelm\\anaconda3\\envs\\mllmrc-gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "import nltk\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from datasets import Dataset\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor is on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check whether CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.rand(3, 3).to(device)\n",
    "print(f'Tensor is on: {x.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "# print cuda version\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abelm\n"
     ]
    }
   ],
   "source": [
    "# check the current working directory\n",
    "print(os.getcwd())\n",
    "os.chdir('C:/Users/abelm/OneDrive/Documents/GitHub/Microsoft-Learn-Location-Mention-Recognition-Challenge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION AND CLEANING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1001136212718088192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EllicottCity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1001136696589631488</td>\n",
       "      <td>Flash floods struck a Maryland city on Sunday,...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1001136950345109504</td>\n",
       "      <td>State of emergency declared for Maryland flood...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1001137334056833024</td>\n",
       "      <td>Other parts of Maryland also saw significant d...</td>\n",
       "      <td>Baltimore Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1001138374923579392</td>\n",
       "      <td>Catastrophic Flooding Slams Ellicott City, Mar...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                                               text  \\\n",
       "0  ID_1001136212718088192                                                NaN   \n",
       "1  ID_1001136696589631488  Flash floods struck a Maryland city on Sunday,...   \n",
       "2  ID_1001136950345109504  State of emergency declared for Maryland flood...   \n",
       "3  ID_1001137334056833024  Other parts of Maryland also saw significant d...   \n",
       "4  ID_1001138374923579392  Catastrophic Flooding Slams Ellicott City, Mar...   \n",
       "\n",
       "                 location  \n",
       "0            EllicottCity  \n",
       "1                Maryland  \n",
       "2                Maryland  \n",
       "3      Baltimore Maryland  \n",
       "4  Ellicott City Maryland  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "trainData = pd.read_csv('data/kaggle/Train_1-new.csv')\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data is: (73072, 3)\n",
      "The number of missing values in the data are: tweet_id        0\n",
      "text        56624\n",
      "location    29612\n",
      "dtype: int64\n",
      "The number of missing values in the data are: tweet_id    0\n",
      "text        0\n",
      "location    0\n",
      "dtype: int64\n",
      "The shape of the data is: (11849, 3)\n"
     ]
    }
   ],
   "source": [
    "# print out shape \n",
    "print(f'The shape of the data is: {trainData.shape}')\n",
    "\n",
    "# print out the number of missing values in each column\n",
    "print(f'The number of missing values in the data are: {trainData.isnull().sum()}')\n",
    "\n",
    "# drop the missing value rows\n",
    "trainData = trainData.dropna()\n",
    "\n",
    "# print out the number of missing values in each column after dropping missing values\n",
    "print(f'The number of missing values in the data are: {trainData.isnull().sum()}')\n",
    "\n",
    "# print out the shape of the data after dropping missing values\n",
    "print(f'The shape of the data is: {trainData.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# text clean helper function\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove mentions (@user) and hashtags (#hashtag)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove special characters, numbers, and punctuations, keeping spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = text.split()  # Split by spaces without tokenizing\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join words back into a single string\n",
    "    return ' '.join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing values in the data are: tweet_id    0\n",
      "text        0\n",
      "location    0\n",
      "dtype: int64\n",
      "The shape of the data is: (11849, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1001136696589631488</td>\n",
       "      <td>Flash floods struck Maryland city Sunday washi...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1001136950345109504</td>\n",
       "      <td>State emergency declared Maryland flooding via</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1001137334056833024</td>\n",
       "      <td>Other parts Maryland also saw significant dama...</td>\n",
       "      <td>Baltimore Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1001138374923579392</td>\n",
       "      <td>Catastrophic Flooding Slams Ellicott City Mary...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID_1001138377717157888</td>\n",
       "      <td>WATCH missing flash devastates Ellicott City M...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                                               text  \\\n",
       "1  ID_1001136696589631488  Flash floods struck Maryland city Sunday washi...   \n",
       "2  ID_1001136950345109504     State emergency declared Maryland flooding via   \n",
       "3  ID_1001137334056833024  Other parts Maryland also saw significant dama...   \n",
       "4  ID_1001138374923579392  Catastrophic Flooding Slams Ellicott City Mary...   \n",
       "5  ID_1001138377717157888  WATCH missing flash devastates Ellicott City M...   \n",
       "\n",
       "                 location  \n",
       "1                Maryland  \n",
       "2                Maryland  \n",
       "3      Baltimore Maryland  \n",
       "4  Ellicott City Maryland  \n",
       "5  Ellicott City Maryland  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the text\n",
    "trainData['text'] = trainData['text'].apply(clean_text)\n",
    "\n",
    "# covert text and location to string\n",
    "trainData['text'] = trainData['text'].astype(str)\n",
    "trainData['location'] = trainData['location'].astype(str)\n",
    "\n",
    "# print out the number of missing values in each column after dropping missing values\n",
    "print(f'The number of missing values in the data are: {trainData.isnull().sum()}')\n",
    "\n",
    "# print out the shape of the data after dropping missing values\n",
    "print(f'The shape of the data is: {trainData.shape}')\n",
    "\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE BIOES FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into train and dev\n",
    "trainData, devData = train_test_split(trainData, test_size=0.2, random_state=42)\n",
    "\n",
    "# save data to a csv tainData\n",
    "trainData.to_csv('data/kaggle/Train-dropna.csv', index=False)\n",
    "\n",
    "# save data to a csv devData\n",
    "devData.to_csv('data/kaggle/Dev-dropna.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and label helper function\n",
    "def tokenize_and_label(text, location):\n",
    "    \"\"\"\n",
    "    Tokenize the tweet text and label tokens using BIOES format for location entities.\n",
    "    \n",
    "    Args:\n",
    "    - text: A string representing the tweet.\n",
    "    - location: A string representing the location entity to tag in the text.\n",
    "    \n",
    "    Returns:\n",
    "    - List of tuples: Each tuple contains a token and its corresponding BIOES tag.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)  # Tokenize the tweet text\n",
    "    labels = ['O'] * len(tokens)  # Initialize with 'O' tags for all tokens\n",
    "    \n",
    "    location_tokens = nltk.word_tokenize(location)  # Tokenize the location string\n",
    "    start_idx = None\n",
    "    \n",
    "    # Find where the location starts in the tokenized text\n",
    "    for i in range(len(tokens) - len(location_tokens) + 1):\n",
    "        if tokens[i:i + len(location_tokens)] == location_tokens:\n",
    "            start_idx = i\n",
    "            break\n",
    "    \n",
    "    # If the location is found, assign the appropriate BIOES tags\n",
    "    if start_idx is not None:\n",
    "        if len(location_tokens) == 1:\n",
    "            labels[start_idx] = 'S-LOC'  # Single-word location\n",
    "        else:\n",
    "            labels[start_idx] = 'B-LOC'  # Beginning of multi-word location\n",
    "            labels[start_idx + len(location_tokens) - 1] = 'E-LOC'  # End of multi-word location\n",
    "            for j in range(start_idx + 1, start_idx + len(location_tokens) - 1):\n",
    "                labels[j] = 'I-LOC'  # Inside of multi-word location\n",
    "    \n",
    "    return list(zip(tokens, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to bioes helper function\n",
    "def convert_to_bioes(csv_file, output_file):\n",
    "    \"\"\"\n",
    "    Convert the text and location data from the CSV to BIOES format and save to a file.\n",
    "    \n",
    "    Args:\n",
    "    - csv_file: Path to the input CSV file containing \"text\" and \"location\" columns.\n",
    "    - output_file: Path to the output file where the BIOES formatted data will be saved.\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Assuming the CSV has columns: \"text\" and \"location\"\n",
    "    bioes_data = []\n",
    "\n",
    "    # Process each row\n",
    "    for index, row in df.iterrows():\n",
    "        tweet_text = row['text']\n",
    "        location = row['location']  # Now it's just a single string, no list needed\n",
    "        \n",
    "        # Check if text or location is NaN or not a string, and skip that row if true\n",
    "        if not isinstance(tweet_text, str) or not isinstance(location, str):\n",
    "            print(f\"Skipping row {index} due to invalid data.\")\n",
    "            continue\n",
    "\n",
    "        # Get the tokens and their BIOES labels\n",
    "        token_labels = tokenize_and_label(tweet_text, location)\n",
    "        bioes_data.extend(token_labels)\n",
    "        bioes_data.append((\"\", \"\"))  # Add a blank line between tweets\n",
    "\n",
    "    # Define the directory from the output file path\n",
    "    directory = os.path.dirname(output_file)\n",
    "\n",
    "    # Check if the directory exists, if not, create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Save the BIOES formatted data to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for token, label in bioes_data:\n",
    "            if token:  # Write token and label\n",
    "                f.write(f\"{token} {label}\\n\")\n",
    "            else:  # Write a blank line between tweets\n",
    "                f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# create BIOES formatted data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m convert_to_bioes(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/kaggle/train-dropna.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/kaggle/BIOES/train_bioes_file.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m convert_to_bioes(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/kaggle/dev-dropna.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/kaggle/BIOES/dev_bioes_file.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m, in \u001b[0;36mconvert_to_bioes\u001b[1;34m(csv_file, output_file)\u001b[0m\n\u001b[0;32m     19\u001b[0m location \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Now it's just a single string, no list needed\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Get the tokens and their BIOES labels\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m token_labels \u001b[38;5;241m=\u001b[39m tokenize_and_label(tweet_text, location)\n\u001b[0;32m     23\u001b[0m bioes_data\u001b[38;5;241m.\u001b[39mextend(token_labels)\n\u001b[0;32m     24\u001b[0m bioes_data\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# Add a blank line between tweets\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m, in \u001b[0;36mtokenize_and_label\u001b[1;34m(text, location)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_and_label\u001b[39m(text, location):\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    Tokenize the tweet text and label tokens using BIOES format for location entities.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    - List of tuples: Each tuple contains a token and its corresponding BIOES tag.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text)  \u001b[38;5;66;03m# Tokenize the tweet text\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)  \u001b[38;5;66;03m# Initialize with 'O' tags for all tokens\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     location_tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(location)  \u001b[38;5;66;03m# Tokenize the location string\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mllmrc-gpu\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mllmrc-gpu\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:120\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mllmrc-gpu\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1280\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mllmrc-gpu\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1340\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1333\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mllmrc-gpu\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1340\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1333\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mllmrc-gpu\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1328\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1327\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mllmrc-gpu\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1457\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1454\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1456\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1458\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mllmrc-gpu\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mllmrc-gpu\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1429\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[0;32m   1428\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1429\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1431\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mllmrc-gpu\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1394\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1392\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1393\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_vars\u001b[38;5;241m.\u001b[39mperiod_context_re()\u001b[38;5;241m.\u001b[39mfinditer(text):\n\u001b[0;32m   1395\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[0;32m   1396\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[0;32m   1397\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "# create BIOES formatted data\n",
    "convert_to_bioes('data/kaggle/train-dropna.csv', 'data/kaggle/BIOES/train_bioes_file.txt')\n",
    "convert_to_bioes('data/kaggle/dev-dropna.csv', 'data/kaggle/BIOES/dev_bioes_file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bioes_file(file_path):\n",
    "    sentences, labels = [], []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            sentence, label = [], []\n",
    "            for line in file:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        word, tag = line.strip().split()\n",
    "                        sentence.append(word)\n",
    "                        label.append(tag)\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping malformed line: {line.strip()}\")\n",
    "                else:\n",
    "                    if sentence and label:\n",
    "                        sentences.append(sentence)\n",
    "                        labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "        return sentences, labels\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"Unicode decoding error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        \n",
    "train_sentences, train_labels = read_bioes_file('data/kaggle/BIOES/train_bioes_file.txt')\n",
    "dev_sentences, dev_labels = read_bioes_file('data/kaggle/BIOES/dev_bioes_file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train sentences: \", len(train_sentences))\n",
    "print(\"Train labels: \", len(train_labels))\n",
    "print(\"Dev sentences: \", len(dev_sentences))\n",
    "print(\"Dev labels: \", len(dev_labels))\n",
    "print()\n",
    "\n",
    "print(\"Train sentences: \\n\", train_sentences[0])\n",
    "print(\"Train labels: \\n\", train_labels[0])\n",
    "print(\"Dev sentences: \\n\", dev_sentences[0])\n",
    "print(\"Dev labels: \\n\", dev_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up label mapping\n",
    "all_labels = set()\n",
    "\n",
    "for labels in train_labels + dev_labels:\n",
    "    all_labels.update(labels)\n",
    "\n",
    "label_list = sorted(list(all_labels))\n",
    "\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train_labels to IDs\n",
    "train_labels_ids = [[label2id[label] for label in sentence_labels] for sentence_labels in train_labels]\n",
    "dev_labels_ids = [[label2id[label] for label in sentence_labels] for sentence_labels in dev_labels]\n",
    "\n",
    "# Example usage\n",
    "print(\"Original first sentence labels:\", train_labels[0])\n",
    "print(\"Converted first sentence label IDs:\", train_labels_ids[0])\n",
    "\n",
    "print(\"Valid length: \", len(train_labels) == len(train_labels_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sentence and label from BIOES to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence_and_labels(sentence, labels):\n",
    "    processed_sentence = []\n",
    "    processed_labels = []\n",
    "\n",
    "    for word, label in zip(sentence, labels):\n",
    "        # Remove words with special characters or numbers\n",
    "        if not re.match(r'^[a-zA-Z]+$', word):\n",
    "            continue\n",
    "\n",
    "        # If the word is not empty after processing, keep it and its label\n",
    "        if word:\n",
    "            processed_sentence.append(word)\n",
    "            processed_labels.append(label)\n",
    "\n",
    "    return processed_sentence, processed_labels\n",
    "\n",
    "# Process the training data\n",
    "processed_train_sentences = []\n",
    "processed_train_labels_ids = []\n",
    "\n",
    "# Process the training data\n",
    "processed_dev_sentences = []\n",
    "processed_dev_labels_ids = []\n",
    "\n",
    "for sentence, labels in zip(train_sentences, train_labels_ids):\n",
    "    proc_sentence, proc_labels = preprocess_sentence_and_labels(sentence, labels)\n",
    "    processed_train_sentences.append(proc_sentence)\n",
    "    processed_train_labels_ids.append(proc_labels)\n",
    "\n",
    "for sentence, labels in zip(dev_sentences, dev_labels_ids):\n",
    "    proc_sentence, proc_labels = preprocess_sentence_and_labels(sentence, labels)\n",
    "    processed_dev_sentences.append(proc_sentence)\n",
    "    processed_dev_labels_ids.append(proc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print an example to compare\n",
    "print(\"Original sentence:\", train_sentences[2])\n",
    "print(\"Original labels:\", train_labels_ids[2])\n",
    "print(\"\\nProcessed sentence:\", processed_train_sentences[2])\n",
    "print(len(processed_train_sentences[2]))\n",
    "print(\"Processed labels:\", processed_train_labels_ids[2])\n",
    "print(len(processed_train_labels_ids[2]))\n",
    "\n",
    "# Print some statistics\n",
    "original_word_count = sum(len(sentence) for sentence in train_sentences)\n",
    "processed_word_count = sum(len(sentence) for sentence in processed_train_sentences)\n",
    "print(f\"\\nOriginal word count: {original_word_count}\")\n",
    "print(f\"Processed word count: {processed_word_count}\")\n",
    "print(f\"Removed {original_word_count - processed_word_count} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "CT_M3_Complete_tokenizer = AutoTokenizer.from_pretrained(\"crisistransformers/CT-M3-Complete\")\n",
    "\n",
    "def tokenize_and_adjust_labels(sentence: List[str], labels: List[int], tokenizer, max_length: int) -> Tuple[List[int], List[int]]:\n",
    "    tokenized_input = tokenizer(\n",
    "        sentence,\n",
    "        is_split_into_words=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Debugging: Print the shape of the tokenized input\n",
    "    print(\"Tokenized input shape:\", tokenized_input[\"input_ids\"].shape)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"][0].tolist())  # Convert to list\n",
    "\n",
    "    # Debugging: Check tokens and their count\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Number of tokens:\", len(tokens))\n",
    "\n",
    "    updated_labels = []\n",
    "    current_label_idx = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in ['<s>', '</s>', '<unk>']:\n",
    "            updated_labels.append(-100)\n",
    "        elif token.endswith('@@'):  # Handle word pieces ending with @@\n",
    "            if current_label_idx < len(labels):\n",
    "                updated_labels.append(labels[current_label_idx])\n",
    "        else:\n",
    "            if current_label_idx < len(labels):\n",
    "                updated_labels.append(labels[current_label_idx])\n",
    "                current_label_idx += 1\n",
    "            else:\n",
    "                updated_labels.append(-100)  # Handle case when there are no more labels\n",
    "\n",
    "    return tokenized_input[\"input_ids\"][0].tolist(), updated_labels  # Return as list\n",
    "\n",
    "# Set the maximum length for tokenization\n",
    "max_length = 2000  # Adjust this as needed\n",
    "\n",
    "# Apply the function to all sentences and labels\n",
    "tokenized_train_inputs = []\n",
    "adjusted_train_labels = []\n",
    "\n",
    "tokenized_dev_inputs = []\n",
    "adjusted_dev_labels = []\n",
    "\n",
    "for sentence, labels in zip(processed_train_sentences, processed_train_labels_ids):\n",
    "    input_ids, adjusted_labels = tokenize_and_adjust_labels(sentence, labels, CT_M3_Complete_tokenizer, max_length)\n",
    "    tokenized_train_inputs.append(input_ids)\n",
    "    adjusted_train_labels.append(adjusted_labels)\n",
    "\n",
    "for sentence, labels in zip(processed_dev_sentences, processed_dev_labels_ids):\n",
    "    input_ids, adjusted_labels = tokenize_and_adjust_labels(sentence, labels, CT_M3_Complete_tokenizer, max_length)\n",
    "    tokenized_dev_inputs.append(input_ids)\n",
    "    adjusted_dev_labels.append(adjusted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print an example to verify\n",
    "print(\"Original sentence:\", processed_train_sentences[2])\n",
    "print(\"Original labels:\", processed_train_labels_ids[2])\n",
    "print(\"\\nTokenized input:\", tokenized_train_inputs[2])\n",
    "print(\"Adjusted labels:\", adjusted_train_labels[2])\n",
    "\n",
    "# Verify lengths\n",
    "print(\"\\nLength of tokenized input:\", len(tokenized_train_inputs[2]))\n",
    "print(\"Length of adjusted labels:\", len(adjusted_train_labels[2]))\n",
    "\n",
    "# Print some statistics\n",
    "original_sentence_count = len(input_ids)\n",
    "tokenized_sentence_count = len(tokenized_train_inputs)\n",
    "print(f\"\\nNumber of original sentences: {original_sentence_count}\")\n",
    "print(f\"Number of tokenized sentences: {tokenized_sentence_count}\")\n",
    "\n",
    "average_original_length = sum(len(s) for s in processed_train_sentences) / original_sentence_count\n",
    "average_tokenized_length = sum(len(s) for s in tokenized_train_inputs) / tokenized_sentence_count\n",
    "print(f\"\\nAverage original sentence length: {average_original_length:.2f}\")\n",
    "print(f\"Average tokenized sentence length: {average_tokenized_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(input_ids), len(adjusted_labels))  # Check lengths here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, labels in zip(processed_train_sentences, processed_train_labels_ids):\n",
    "    input_ids, adjusted_labels = tokenize_and_adjust_labels(sentence, labels, CT_M3_Complete_tokenizer, max_length=2000)\n",
    "    print(f\"Input IDs shape: {len(input_ids)}\")  # Print length of input IDs\n",
    "    print(f\"Adjusted labels shape: {len(adjusted_labels)}\")  # Print length of adjusted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datasets\n",
    "tokenized_train = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_train_inputs,\n",
    "    \"labels\": adjusted_train_labels\n",
    "})\n",
    "tokenized_dev = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_dev_inputs,\n",
    "    \"labels\": adjusted_dev_labels\n",
    "})\n",
    "\n",
    "# Set up label mapping\n",
    "all_labels = set()\n",
    "\n",
    "for labels in train_labels + dev_labels:\n",
    "    all_labels.update(labels)\n",
    "\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "labels = sorted(list(all_labels))\n",
    "\n",
    "label_list = sorted(list(all_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"crisistransformers/CT-M3-Complete\"\n",
    "\n",
    "# Update model configuration\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = len(label_list)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "\n",
    "CT_M3_Complete_model = AutoModelForTokenClassification.from_pretrained(model_name, config=config)\n",
    "CT_M3_Complete_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    print(f\"Predictions shape: {predictions.shape}, Labels shape: {labels.shape}\")\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = precision_recall_fscore_support(sum(true_labels, []), sum(true_predictions, []), average='weighted')\n",
    "    return {\n",
    "        \"precision\": results[0],\n",
    "        \"recall\": results[1],\n",
    "        \"f1\": results[2],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model and tokenizer\n",
    "model_name = \"crisistransformers/CT-M3-Complete\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = len(label_list)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=CT_M3_Complete_tokenizer, padding=True)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, model, inputs):\n",
    "        # Print input shapes here\n",
    "        print(f\"Inputs: {inputs['input_ids'].shape}, Labels: {inputs['labels'].shape}\")\n",
    "        return super().training_step(model, inputs)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"data/kaggle/CrisisTransformers\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Reduced batch size\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"adamw_torch\",  # Use PyTorch's AdamW implementation\n",
    "    logging_steps=100,  # Reduce logging frequency\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "    report_to='none',  # Disable logging to wandb\n",
    ")\n",
    "\n",
    "# Set up trainer\n",
    "CustomTrainer = Trainer(\n",
    "    model=CT_M3_Complete_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    tokenizer=CT_M3_Complete_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training dataset size: {len(tokenized_train_inputs)}\")\n",
    "print(f\"First training input shape: {len(tokenized_train_inputs[0])}\")\n",
    "print(f\"First training label shape: {len(adjusted_train_labels[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "CustomTrainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = CustomTrainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training\n",
    "output_dir = \"data/kaggle/working/results\"\n",
    "\n",
    "# Save the model\n",
    "CustomTrainer.save_model(output_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "CT_M3_Complete_tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save training arguments\n",
    "with open(f\"{output_dir}/training_args.json\", 'w') as f:\n",
    "    json.dump(training_args.to_dict(), f)\n",
    "\n",
    "# Save label mappings\n",
    "with open(f\"{output_dir}/label_mappings.json\", 'w') as f:\n",
    "    json.dump({\"label2id\": label2id, \"id2label\": id2label}, f)\n",
    "\n",
    "print(f\"Model and associated files saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"data/kaggle/working/results\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"data/kaggle/working/results\")\n",
    "\n",
    "# Load label mappings\n",
    "with open(\"data/kaggle/working/results/label_mappings.json\", 'r') as f:\n",
    "    label_mappings = json.load(f)\n",
    "\n",
    "id2label = label_mappings[\"id2label\"]\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_subwords_and_locations(tokens_and_labels):\n",
    "    merged_words = []\n",
    "    merged_labels = []\n",
    "    current_word = []\n",
    "    current_labels = []\n",
    "    location_buffer = []\n",
    "\n",
    "    for token, label in tokens_and_labels:\n",
    "        if token.endswith('@@'):\n",
    "            current_word.append(token[:-2])  # Remove '@@'\n",
    "            current_labels.append(label)\n",
    "        else:\n",
    "            current_word.append(token)\n",
    "            current_labels.append(label)\n",
    "\n",
    "            # Merge subwords\n",
    "            merged_word = ''.join(current_word)\n",
    "\n",
    "            # Voting for the label\n",
    "            if len(set(current_labels)) == 1:\n",
    "                merged_label = current_labels[0]\n",
    "            else:\n",
    "                priority_order = ['B-LOC', 'I-LOC', 'E-LOC', 'S-LOC', 'O']\n",
    "                merged_label = next(label for label in priority_order if label in current_labels)\n",
    "\n",
    "            # Handle location merging\n",
    "            if merged_label.endswith('-LOC'):\n",
    "                if merged_label == 'B-LOC' or merged_label == 'S-LOC':\n",
    "                    if location_buffer:\n",
    "                        merged_words.append(' '.join(location_buffer))\n",
    "                        merged_labels.append('B-LOC')\n",
    "                        location_buffer = []\n",
    "                    location_buffer.append(merged_word)\n",
    "                elif merged_label == 'I-LOC' or merged_label == 'E-LOC':\n",
    "                    location_buffer.append(merged_word)\n",
    "                    if merged_label == 'E-LOC':\n",
    "                        merged_words.append(' '.join(location_buffer))\n",
    "                        merged_labels.append('B-LOC')\n",
    "                        location_buffer = []\n",
    "            else:\n",
    "                if location_buffer:\n",
    "                    merged_words.append(' '.join(location_buffer))\n",
    "                    merged_labels.append('B-LOC')\n",
    "                    location_buffer = []\n",
    "                merged_words.append(merged_word)\n",
    "                merged_labels.append(merged_label)\n",
    "\n",
    "            # Reset for next word\n",
    "            current_word = []\n",
    "            current_labels = []\n",
    "\n",
    "    # Handle any remaining location in the buffer\n",
    "    if location_buffer:\n",
    "        merged_words.append(' '.join(location_buffer))\n",
    "        merged_labels.append('B-LOC')\n",
    "\n",
    "    return list(zip(merged_words, merged_labels))\n",
    "\n",
    "# # Usage\n",
    "# merged_result = merge_subwords_and_locations(predicted_tokens)\n",
    "\n",
    "# # Extract locations\n",
    "# locations = [word for word, label in merged_result if label == 'B-LOC']\n",
    "# print(\"\\nExtracted locations:\", locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
    "\n",
    "    tokens = []\n",
    "    predicted_tokens = []\n",
    "\n",
    "    locations = []\n",
    "    current_location = []\n",
    "\n",
    "    for token, prediction in zip(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]), predictions[0]):\n",
    "        if int(prediction) == 0:  # Beginning of a new location\n",
    "            current_location = [token]\n",
    "        elif int(prediction) == 2:  # Inside a location\n",
    "            if current_location:  # Make sure we started a location\n",
    "                current_location.append(token)\n",
    "        elif int(prediction) == 1:  # End of a location\n",
    "            if current_location:  # Make sure we're inside a location\n",
    "                current_location.append(token)\n",
    "                locations.append(\" \".join(current_location))\n",
    "                current_location = []\n",
    "        elif int(prediction) == 4:  # Single token location\n",
    "            locations.append(token)\n",
    "        else:\n",
    "            current_location = []  # Reset if prediction is 'O' or anything else\n",
    "\n",
    "        # Remove special tokens and clean up the text\n",
    "        if token not in ['<s>', '</s>', '<unk>']:\n",
    "            cleaned_token = token[1:] if token.startswith('') else token\n",
    "\n",
    "            if token.startswith('##'):\n",
    "                if predicted_tokens:\n",
    "                    predicted_tokens[-1] = (predicted_tokens[-1][0] + cleaned_token, predicted_tokens[-1][1])\n",
    "                continue\n",
    "\n",
    "            tokens.append(cleaned_token)\n",
    "            predicted_tokens.append((cleaned_token, id2label[str(prediction.item())]))\n",
    "\n",
    "    # Usage\n",
    "    merged_result = merge_subwords_and_locations(predicted_tokens)\n",
    "\n",
    "    # Extract locations\n",
    "    locations = [word for word, label in merged_result if label == 'B-LOC']\n",
    "\n",
    "    # Extract unique locations and sort alphabetically\n",
    "    unique_locations = sorted(set(locations))\n",
    "\n",
    "    return unique_locations, tokens, predictions, predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/kaggle/test-new.csv\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.data.path.append('/usr/share/nltk_data/')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '<URL>', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\./\\-_]', '', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "#     # Lemmatize\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into a string\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Apply preprocessing to each text in your dataset\n",
    "test['processed_text'] = test['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = []\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    if index % 100 == 0:\n",
    "        print(f\"Processing row {index}\")\n",
    "\n",
    "    id = row['tweet_id']\n",
    "    processed_text = row['processed_text']\n",
    "\n",
    "    unique_locations, tokens, predictions, predicted_tokens = predict(processed_text)\n",
    "\n",
    "    # Join locations with space, or use a single space if no locations\n",
    "    locations_string = ' '.join(unique_locations) if unique_locations else ' '\n",
    "\n",
    "    submission.append({'ID': id, 'Locations': locations_string})\n",
    "\n",
    "# Create DataFrame from submission list\n",
    "submission_df = pd.DataFrame(submission)\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('data/kaggle/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
