{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from thinc.api import prefer_gpu\n",
    "\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.rand(3, 3).to(device)\n",
    "print(f'Tensor is on: {x.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prefer_gpu():\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require spacy to use GPU\n",
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the current working directory\n",
    "#os.chdir('C:/Users/lamem/OneDrive/Documents/GHD/Microsoft-Learn-Location-Mention-Recognition-Challenge')\n",
    "os.chdir('C:/Users/abelm/OneDrive/Documents/GitHub/Microsoft-Learn-Location-Mention-Recognition-Challenge')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to clean text\n",
    "def clean_text(text):\n",
    "    \n",
    "    # remove links\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # remove html tags\n",
    "    text = re.sub('r<.*?>', '', text)\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
    "    # remove symbols\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    \n",
    "    # return cleaned text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data\n",
    "train = pd.read_csv(\"lewa/Train_1.csv\")\n",
    "\n",
    "# drop empty text columns\n",
    "train = train.dropna(subset=['text'])\n",
    "\n",
    "# clean text\n",
    "train['text'] = train['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "# add an empty string to the location column with missing values (Nan)\n",
    "train['location'] = train['location'].fillna(' ')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to spaCy's training format\n",
    "TRAIN_DATA = []\n",
    "for index, row in train.iterrows():\n",
    "    text = row['text']\n",
    "    location_str = row['location']\n",
    "    \n",
    "    if location_str:\n",
    "        locations = location_str.split()\n",
    "        entities = []\n",
    "        used_indices = set()  # Track used indices to avoid overlaps\n",
    "        \n",
    "        for location in locations:\n",
    "            start_idx = text.find(location)\n",
    "            end_idx = start_idx + len(location)\n",
    "            if start_idx != -1 and not any(idx in used_indices for idx in range(start_idx, end_idx)):\n",
    "                entities.append((start_idx, end_idx, \"GPE\"))\n",
    "                used_indices.update(range(start_idx, end_idx))  # Mark indices as used\n",
    "\n",
    "        if entities:\n",
    "            TRAIN_DATA.append((text, {\"entities\": entities}))\n",
    "    else:\n",
    "        TRAIN_DATA.append((text, {\"entities\": []}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPACY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create the training pipeline\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "ner.add_label(\"GPE\")  # Add your custom labels here\n",
    "\n",
    "# Step 3: Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(TRAIN_DATA, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Fine-tune the model with monitoring\n",
    "# Set parameters for the training\n",
    "n_iter = 10  # Number of training iterations\n",
    "batch_sizes = compounding(1.0, 4.0, 1.001)\n",
    "\n",
    "# Track training losses and evaluation metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "val_gold_total = 0\n",
    "\n",
    "# Start training\n",
    "for epoch in range(n_iter):\n",
    "    random.shuffle(train_data)  # Shuffle the training data\n",
    "\n",
    "    losses = {}\n",
    "    # Create minibatches for training\n",
    "    for batch in minibatch(train_data, size=8):  # Adjust size as needed\n",
    "        for text, annotations in batch:\n",
    "            # Create an Example object\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            \n",
    "            # Update the model and collect losses\n",
    "            nlp.update([example], drop=0.5, losses=losses)  # Dropout for regularization\n",
    "    \n",
    "    # Append the training loss for monitoring\n",
    "    train_losses.append(losses[\"ner\"])  # Append NER loss\n",
    "\n",
    "    # Validation step\n",
    "    val_loss = 0  # Reset validation loss for each epoch\n",
    "    for text, annotations in val_data:\n",
    "        doc = nlp(text)  # Process the document\n",
    "        predicted_entities = {(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents}\n",
    "        gold_entities = {(start, end, label) for start, end, label in annotations.get(\"entities\")}\n",
    "\n",
    "        val_correct += len(predicted_entities & gold_entities)  # Count the correctly predicted entities\n",
    "        val_total += len(predicted_entities)  # Count the number of entities detected by the model\n",
    "        val_gold_total += len(gold_entities)  # Count the number of gold-standard entities\n",
    "\n",
    "    # Calculate precision, recall, and F1-score for the validation set\n",
    "    precision = val_correct / val_total if val_total > 0 else 0\n",
    "    recall = val_correct / val_gold_total if val_gold_total > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f\"Epoch {epoch + 1}/{n_iter}, Train Loss: {losses.get('ner', 0):.3f}, \"\n",
    "        f\"Val Precision: {precision:.2f}, Val Recall: {recall:.2f}, Val F1 Score: {f1_score:.2f}\")\n",
    "    \n",
    "    \n",
    "print(\"Model training complete and saved as 'fine_tuned_model'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save the fine-tuned model\n",
    "nlp.to_disk(\"lewa/spacy/fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUBMISSION BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test data\n",
    "test = pd.read_csv(\"lewa/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text\n",
    "test['text'] = test['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract locations from text using spaCy\n",
    "def extract_locations(text):\n",
    "    doc = nlp(text)\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ == 'GPE']  # GPE stands for Geopolitical Entity (locations)\n",
    "    return ' '.join(locations) if locations else ' ' # join locations with space if many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with extracted locations\n",
    "submission = pd.DataFrame({\n",
    "    'tweet_id': test['tweet_id'],\n",
    "    'locations': test['text'].apply(extract_locations)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()\n",
    "# save the submission to a csv\n",
    "submission.to_csv('lewa/spacy-baseline-submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINED TUNED SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fine tuned model\n",
    "nlp = spacy.load(\"lewa/spacy/fine_tuned_model\")\n",
    "\n",
    "# Function to extract locations from text using spaCy\n",
    "def extract_locations(text):\n",
    "    doc = nlp(text)\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ == 'GPE']  # GPE stands for Geopolitical Entity (locations)\n",
    "    return ' '.join(locations) if locations else ' ' # join locations with space if many\n",
    "\n",
    "# Create a new dataframe with extracted locations\n",
    "submission = pd.DataFrame({\n",
    "    'tweet_id': test['tweet_id'],\n",
    "    'locations': test['text'].apply(extract_locations)\n",
    "})  \n",
    "\n",
    "submission.head()\n",
    "# save the submission to a csv\n",
    "submission.to_csv('lewa/spacy-fine-tuned-submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
