{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abelm\\anaconda3\\envs\\mllmrc-gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "import nltk\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from datasets import Dataset\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuda and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor is on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check whether CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.rand(3, 3).to(device)\n",
    "print(f'Tensor is on: {x.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "# print cuda version\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abelm\\OneDrive\\Documents\\GitHub\\Microsoft-Learn-Location-Mention-Recognition-Challenge\n"
     ]
    }
   ],
   "source": [
    "# check the current working directory\n",
    "#os.chdir('C:/Users/lamem/OneDrive/Documents/GHD/Microsoft-Learn-Location-Mention-Recognition-Challenge')\n",
    "os.chdir('C:/Users/abelm/OneDrive/Documents/GitHub/Microsoft-Learn-Location-Mention-Recognition-Challenge')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1001136212718088192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EllicottCity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1001136696589631488</td>\n",
       "      <td>Flash floods struck a Maryland city on Sunday,...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1001136950345109504</td>\n",
       "      <td>State of emergency declared for Maryland flood...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1001137334056833024</td>\n",
       "      <td>Other parts of Maryland also saw significant d...</td>\n",
       "      <td>Baltimore Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1001138374923579392</td>\n",
       "      <td>Catastrophic Flooding Slams Ellicott City, Mar...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                                               text  \\\n",
       "0  ID_1001136212718088192                                                NaN   \n",
       "1  ID_1001136696589631488  Flash floods struck a Maryland city on Sunday,...   \n",
       "2  ID_1001136950345109504  State of emergency declared for Maryland flood...   \n",
       "3  ID_1001137334056833024  Other parts of Maryland also saw significant d...   \n",
       "4  ID_1001138374923579392  Catastrophic Flooding Slams Ellicott City, Mar...   \n",
       "\n",
       "                 location  \n",
       "0            EllicottCity  \n",
       "1                Maryland  \n",
       "2                Maryland  \n",
       "3      Baltimore Maryland  \n",
       "4  Ellicott City Maryland  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = pd.read_csv(\"lewa/Train_1.csv\")\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# text clean helper function\n",
    "def clean_text(text):\n",
    "    # remove #'s and separate words based on capital letters and small letters\n",
    "    # text = ' '.join(re.findall(r'[A-Z]?[a-z]+|[a-z]+|[A-Z]+(?=[A-Z][a-z])|[A-Z]+|\\d+', text.replace('#', ' ')))\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # remove mentions only\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove special characters, numbers, and punctuations, keeping spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = text.split()  # Split by spaces without tokenizing\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join words back into a single string\n",
    "    return ' '.join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1001136696589631488</td>\n",
       "      <td>Flash floods struck Maryland city Sunday washi...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1001136950345109504</td>\n",
       "      <td>State emergency declared Maryland flooding via</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1001137334056833024</td>\n",
       "      <td>Other parts Maryland also saw significant dama...</td>\n",
       "      <td>Baltimore Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1001138374923579392</td>\n",
       "      <td>Catastrophic Flooding Slams Ellicott City Mary...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID_1001138377717157888</td>\n",
       "      <td>WATCH missing flash FLOODING devastates Ellico...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                                               text  \\\n",
       "1  ID_1001136696589631488  Flash floods struck Maryland city Sunday washi...   \n",
       "2  ID_1001136950345109504     State emergency declared Maryland flooding via   \n",
       "3  ID_1001137334056833024  Other parts Maryland also saw significant dama...   \n",
       "4  ID_1001138374923579392  Catastrophic Flooding Slams Ellicott City Mary...   \n",
       "5  ID_1001138377717157888  WATCH missing flash FLOODING devastates Ellico...   \n",
       "\n",
       "                 location  \n",
       "1                Maryland  \n",
       "2                Maryland  \n",
       "3      Baltimore Maryland  \n",
       "4  Ellicott City Maryland  \n",
       "5  Ellicott City Maryland  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your  \n",
    "df_t = new.copy() \n",
    "df_t = df_t.dropna(subset = \"text\")\n",
    "# clean the text\n",
    "df_t['text'] = df_t['text'].apply(clean_text)\n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16448, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into train and dev\n",
    "trainData, devData = train_test_split(df_t, test_size=0.15, random_state=42)\n",
    "\n",
    "# save data to a csv tainData\n",
    "trainData.to_csv('lewa/Train-dropna.csv', index=False)\n",
    "\n",
    "# save data to a csv devData\n",
    "devData.to_csv('lewa/Dev-dropna.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def csv_to_json(csv_file_path, json_file_path):\n",
    "    # Open the CSV file and create a list of dictionaries\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        data = [row for row in csv_reader]\n",
    "\n",
    "    # Write the list of dictionaries to a JSON file\n",
    "    with open(json_file_path, mode='w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "#csv_file_path = 'testData/Train-dropna.csv'  # Path to your CSV file\n",
    "#json_file_path = 'testData/output_file.json'  # Path where the JSON will be saved\n",
    "csv_to_json('lewa/Train-dropna.csv', 'lewa/train_file.json')\n",
    "csv_to_json('lewa/Dev-dropna.csv', 'lewa/dev_file.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDENTIFY location from sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# normaulize and preserve case\n",
    "def normalize_and_preserve_case(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    cleaned_tokens = []\n",
    "    for token in doc:\n",
    "        # Keep the original casing and remove punctuation\n",
    "        if token.text not in string.punctuation:\n",
    "            cleaned_tokens.append(token.lemma_)\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# Function to extract locations as entities after preprocessing\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'GPE':  # 'GPE' is for locations\n",
    "            entities.append({\n",
    "                'entity': ent.text,\n",
    "                'label': ent.label_,\n",
    "                'start_char': ent.start_char,\n",
    "                'end_char': ent.end_char\n",
    "            })\n",
    "    return entities\n",
    "\n",
    "# Function to process a JSON file\n",
    "def process_json_file(file_path):\n",
    "    # Load JSON data\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert the JSON data to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Preprocess text and extract entities\n",
    "    df['cleaned_text'] = df['text'].apply(normalize_and_preserve_case, nlp=nlp)\n",
    "    df['entities'] = df['cleaned_text'].apply(extract_entities)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Paths to your two JSON files\n",
    "file_path_1 = 'lewa/train_file.json'\n",
    "file_path_2 = 'lewa/dev_file.json'\n",
    "\n",
    "# Process both files\n",
    "df1 = process_json_file(file_path_1)\n",
    "df2 = process_json_file(file_path_2)\n",
    "\n",
    "\n",
    "# If you want to save them separately, you can use the individual DataFrames:\n",
    "# Save the updated data for file1\n",
    "with open('lewa/train_entities.json', 'w') as f:\n",
    "    json.dump(df1.to_dict(orient='records'), f, indent=4)\n",
    "\n",
    "# Save the updated data for file2\n",
    "with open('lewa/dev_entities.json', 'w') as f:\n",
    "    json.dump(df2.to_dict(orient='records'), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGE THE EXTRACTED LOCATIONS WITH EXISTING LOCATION COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to merge location and entities, taking file paths for input and output\n",
    "def merge_entities(input_file_path, output_file_path):\n",
    "    # Read the unmerged JSON file\n",
    "    with open(input_file_path, 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "    \n",
    "    for entry in data:\n",
    "        location = entry.get(\"location\")\n",
    "        entities = entry.get(\"entities\", [])\n",
    "        \n",
    "        # Create the merged_entities field with only entities present in the entities list\n",
    "        entry[\"merged_entities\"] = [ent[\"entity\"] for ent in entities]\n",
    "\n",
    "        merged_entities = [ent[\"entity\"] for ent in entities]\n",
    "\n",
    "        # If location is present and valid, merge it with entities\n",
    "        if location:\n",
    "            merged_entities.append(location)\n",
    "\n",
    "        # Update the entry with merged entities\n",
    "        entry[\"merged_entities\"] = merged_entities\n",
    "    \n",
    "    # Write the updated data to the output JSON file\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_entities0(input_file_path, output_file_path):\n",
    "    # Read the unmerged JSON file\n",
    "    with open(input_file_path, 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "    \n",
    "    for entry in data:\n",
    "        location = entry.get(\"location\")\n",
    "        \n",
    "        # Create the merged_entities field with only the location\n",
    "        entry[\"merged_entities\"] = [location] if location else []\n",
    "    \n",
    "    # Write the updated data to the output JSON file\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your two JSON files\n",
    "input_file_path = 'lewa/train_entities.json'\n",
    "output_file_path = 'lewa/train_merged_entities.json'\n",
    "\n",
    "# Merge entities for the first file\n",
    "merge_entities0(input_file_path, output_file_path)\n",
    "\n",
    "# Paths to your two JSON files\n",
    "input_file_path = 'lewa/dev_entities.json'\n",
    "output_file_path = 'lewa/dev_merged_entities.json'\n",
    "\n",
    "# Merge entities for the first file\n",
    "merge_entities0(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIOES TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to read JSON file and return a DataFrame with specific columns\n",
    "def json_to_csv(input_file_path, output_csv_path):\n",
    "    # Read the JSON file\n",
    "    with open(input_file_path, 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "    \n",
    "    # Create a list to hold the data for the DataFrame\n",
    "    rows = []\n",
    "    \n",
    "    for entry in data:\n",
    "        tweet_id = entry.get(\"tweet_id\")\n",
    "        cleaned_text = entry.get(\"cleaned_text\")\n",
    "        merged_entities = entry.get(\"merged_entities\", [])\n",
    "        \n",
    "        # Convert the list of entities into a single string (join them with a comma and space)\n",
    "        location = ' '.join(merged_entities) if merged_entities else None\n",
    "        \n",
    "        # Add the row to the list\n",
    "        rows.append({\n",
    "            \"tweet_id\": tweet_id,\n",
    "            \"text\": cleaned_text,  # Rename cleaned_text to text\n",
    "            \"location\": location  # Convert merged_entities to a plain text string\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame from the list of rows\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save merged_train_entities to a csv\n",
    "train_file_path = 'lewa/train_merged_entities.json'\n",
    "train_csv_path = 'lewa/train_merged_entities.csv'\n",
    "json_to_csv(train_file_path, train_csv_path)\n",
    "\n",
    "# save merged_dev_entities to a csv\n",
    "dev_file_path = 'lewa/dev_merged_entities.json' \n",
    "dev_csv_path = 'lewa/dev_merged_entities.csv'\n",
    "json_to_csv(dev_file_path, dev_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bioes_tagging(text, merged_entities):\n",
    "    \"\"\"\n",
    "    Apply BIOES tagging to a tokenized text based on merged entity spans.\n",
    "    \n",
    "    :param text: List of tokenized words.\n",
    "    :param merged_entities: List of merged location entities in the sentence (already a list).\n",
    "    :return: List of tuples (word, tag) in BIOES format.\n",
    "    \"\"\"\n",
    "    tags = ['O'] * len(text)\n",
    "    \n",
    "    # Process each merged entity (location)\n",
    "    for entity in merged_entities:\n",
    "        entity_tokens = entity.split()  # Assuming each entity is a string of tokens\n",
    "        entity_length = len(entity_tokens)\n",
    "\n",
    "        # Apply BIOES tags for each token in the entity\n",
    "        for i, token in enumerate(entity_tokens):\n",
    "            try:\n",
    "                token_index = text.index(token)\n",
    "            except ValueError:\n",
    "                continue  # Skip tokens not found in the sentence\n",
    "\n",
    "            if entity_length == 1:\n",
    "                tags[token_index] = 'S-LOC'\n",
    "            elif i == 0:\n",
    "                tags[token_index] = 'B-LOC'\n",
    "            elif i == entity_length - 1:\n",
    "                tags[token_index] = 'E-LOC'\n",
    "            else:\n",
    "                tags[token_index] = 'I-LOC'\n",
    "    \n",
    "    return list(zip(text, tags))\n",
    "\n",
    "def process_json(input_file, output_file):\n",
    "    # Read the JSON file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    output_data = []\n",
    "\n",
    "    # Process each entry in the JSON file\n",
    "    for entry in data:\n",
    "        text = entry['text']  # Assuming 'text' field has the sentence\n",
    "        merged_entities = entry.get('merged_entities', [])  # No split needed, it's already a list\n",
    "        \n",
    "        # Tokenize the text (basic tokenization)\n",
    "        tokens = text.split()\n",
    "\n",
    "        # Apply BIOES tagging\n",
    "        tagged_tokens = bioes_tagging(tokens, merged_entities)\n",
    "        \n",
    "        # Prepare output format\n",
    "        for token, tag in tagged_tokens:\n",
    "            output_data.append(f\"{token}\\t{tag}\")\n",
    "        output_data.append(\"\")  # New line after each sentence\n",
    "\n",
    "    # Write the output to a text file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(output_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "process_json('lewa/train_merged_entities.json', 'lewa/train_bioes.txt')\n",
    "process_json('lewa/dev_merged_entities.json', 'lewa/dev_bioes.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bioes_file(file_path):\n",
    "    sentences, labels = [], []\n",
    "    with open(file_path, 'r') as file:\n",
    "        sentence, label = [], []\n",
    "        for line in file:\n",
    "            if line.strip():\n",
    "                word, tag = line.strip().split()\n",
    "                sentence.append(word)\n",
    "                label.append(tag)\n",
    "            else:\n",
    "                sentences.append(sentence)\n",
    "                labels.append(label)\n",
    "                sentence, label = [], []\n",
    "    return sentences, labels\n",
    "\n",
    "# read the bioes files\n",
    "train_sentences, train_labels = read_bioes_file('lewa/sidechick-bioes-files/train_bioes_mix.txt')\n",
    "dev_sentences, dev_labels = read_bioes_file('lewa/sidechick-bioes-files/dev_bioes_mix.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences:  13157\n",
      "Train labels:  13157\n",
      "Dev sentences:  3289\n",
      "Dev labels:  3289\n",
      "\n",
      "Train sentences: \n",
      " ['Here', 'US', 'Thousands', 'people', 'homeless', 'due', 'fires', 'California', 'These', 'people', 'fellow', 'citizens', 'need', 'help', 'My', 'resources', 'There', 'homeless', 'Vets', 'citizenry', 'needs', 'take', 'care', 'resources', 'Not', 'foreigners']\n",
      "Train labels: \n",
      " ['O', 'S-LOC', 'O', 'O', 'O', 'O', 'O', 'S-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Dev sentences: \n",
      " ['Were', 'collecting', 'supplies', 'Donate', 'items', 'local', 'store', 'FHurricane', 'Maria', 'hurricanerelief', 'Unitedfor', 'P', 'R', 'Build', 'Puerto', 'Rico']\n",
      "Dev labels: \n",
      " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-LOC', 'S-LOC']\n"
     ]
    }
   ],
   "source": [
    "print(\"Train sentences: \", len(train_sentences))\n",
    "print(\"Train labels: \", len(train_labels))\n",
    "print(\"Dev sentences: \", len(dev_sentences))\n",
    "print(\"Dev labels: \", len(dev_labels))\n",
    "print()\n",
    "\n",
    "print(\"Train sentences: \\n\", train_sentences[0])\n",
    "print(\"Train labels: \\n\", train_labels[0])\n",
    "print(\"Dev sentences: \\n\", dev_sentences[0])\n",
    "print(\"Dev labels: \\n\", dev_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-LOC': 0, 'E-LOC': 1, 'I-LOC': 2, 'O': 3, 'S-LOC': 4}\n",
      "{0: 'B-LOC', 1: 'E-LOC', 2: 'I-LOC', 3: 'O', 4: 'S-LOC'}\n"
     ]
    }
   ],
   "source": [
    "# Set up label mapping\n",
    "all_labels = set()\n",
    "\n",
    "for labels in train_labels + dev_labels:\n",
    "    all_labels.update(labels)\n",
    "\n",
    "label_list = sorted(list(all_labels))\n",
    "\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original first sentence labels: ['O', 'S-LOC', 'O', 'O', 'O', 'O', 'O', 'S-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Converted first sentence label IDs: [3, 4, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Valid length:  True\n"
     ]
    }
   ],
   "source": [
    "# Convert train_labels to IDs\n",
    "train_labels_ids = [[label2id[label] for label in sentence_labels] for sentence_labels in train_labels]\n",
    "dev_labels_ids = [[label2id[label] for label in sentence_labels] for sentence_labels in dev_labels]\n",
    "\n",
    "# Example usage\n",
    "print(\"Original first sentence labels:\", train_labels[0])\n",
    "print(\"Converted first sentence label IDs:\", train_labels_ids[0])\n",
    "\n",
    "print(\"Valid length: \", len(train_labels) == len(train_labels_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess senstence and label from BIOES to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence_and_labels(sentence, labels):\n",
    "    processed_sentence = []\n",
    "    processed_labels = []\n",
    "\n",
    "    for word, label in zip(sentence, labels):\n",
    "        # Remove words with special characters or numbers\n",
    "        if not re.match(r'^[a-zA-Z]+$', word):\n",
    "            continue\n",
    "\n",
    "        # If the word is not empty after processing, keep it and its label\n",
    "        if word:\n",
    "            processed_sentence.append(word)\n",
    "            processed_labels.append(label)\n",
    "\n",
    "    return processed_sentence, processed_labels\n",
    "\n",
    "# Process the training data\n",
    "processed_train_sentences = []\n",
    "processed_train_labels_ids = []\n",
    "\n",
    "# Process the training data\n",
    "processed_dev_sentences = []\n",
    "processed_dev_labels_ids = []\n",
    "\n",
    "for sentence, labels in zip(train_sentences, train_labels_ids):\n",
    "    proc_sentence, proc_labels = preprocess_sentence_and_labels(sentence, labels)\n",
    "    processed_train_sentences.append(proc_sentence)\n",
    "    processed_train_labels_ids.append(proc_labels)\n",
    "\n",
    "for sentence, labels in zip(dev_sentences, dev_labels_ids):\n",
    "    proc_sentence, proc_labels = preprocess_sentence_and_labels(sentence, labels)\n",
    "    processed_dev_sentences.append(proc_sentence)\n",
    "    processed_dev_labels_ids.append(proc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: ['How', 'help', 'survivors', 'massive', 'earthquake', 'Ecuador', 'Disaster', 'Relief', 'Donate']\n",
      "Original labels: [3, 3, 3, 3, 3, 4, 3, 3, 3]\n",
      "\n",
      "Processed sentence: ['How', 'help', 'survivors', 'massive', 'earthquake', 'Ecuador', 'Disaster', 'Relief', 'Donate']\n",
      "9\n",
      "Processed labels: [3, 3, 3, 3, 3, 4, 3, 3, 3]\n",
      "9\n",
      "\n",
      "Original word count: 195084\n",
      "Processed word count: 195084\n",
      "Removed 0 words\n"
     ]
    }
   ],
   "source": [
    "# Print an example to compare\n",
    "print(\"Original sentence:\", train_sentences[2])\n",
    "print(\"Original labels:\", train_labels_ids[2])\n",
    "print(\"\\nProcessed sentence:\", processed_train_sentences[2])\n",
    "print(len(processed_train_sentences[2]))\n",
    "print(\"Processed labels:\", processed_train_labels_ids[2])\n",
    "print(len(processed_train_labels_ids[2]))\n",
    "\n",
    "# Print some statistics\n",
    "original_word_count = sum(len(sentence) for sentence in train_sentences)\n",
    "processed_word_count = sum(len(sentence) for sentence in processed_train_sentences)\n",
    "print(f\"\\nOriginal word count: {original_word_count}\")\n",
    "print(f\"Processed word count: {processed_word_count}\")\n",
    "print(f\"Removed {original_word_count - processed_word_count} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abelm\\anaconda3\\envs\\mllmrc-gpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "CT_M3_Complete_tokenizer = AutoTokenizer.from_pretrained(\"crisistransformers/CT-M3-Complete\")\n",
    "\n",
    "def tokenize_and_adjust_labels(sentence: List[str], labels: List[int], tokenizer) -> Tuple[List[int], List[int]]:\n",
    "    tokenized_input = tokenizer(sentence, is_split_into_words=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "\n",
    "    updated_labels = []\n",
    "    current_label_idx = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in ['<s>', '</s>', '<unk>']:\n",
    "            updated_labels.append(-100)\n",
    "        elif token.endswith('@@'):  # Handle word pieces ending with @@\n",
    "            updated_labels.append(labels[current_label_idx])\n",
    "        else:\n",
    "            updated_labels.append(labels[current_label_idx])\n",
    "            current_label_idx += 1\n",
    "\n",
    "    return tokenized_input[\"input_ids\"], updated_labels\n",
    "\n",
    "# Apply the function to all sentences and labels\n",
    "tokenized_train_inputs = []\n",
    "adjusted_train_labels = []\n",
    "\n",
    "tokenized_dev_inputs = []\n",
    "adjusted_dev_labels = []\n",
    "\n",
    "for sentence, labels in zip(processed_train_sentences, processed_train_labels_ids):\n",
    "    input_ids, adjusted_labels = tokenize_and_adjust_labels(sentence, labels, CT_M3_Complete_tokenizer)\n",
    "    tokenized_train_inputs.append(input_ids)\n",
    "    adjusted_train_labels.append(adjusted_labels)\n",
    "\n",
    "for sentence, labels in zip(processed_dev_sentences, processed_dev_labels_ids):\n",
    "    input_ids, adjusted_labels = tokenize_and_adjust_labels(sentence, labels, CT_M3_Complete_tokenizer)\n",
    "    tokenized_dev_inputs.append(input_ids)\n",
    "    adjusted_dev_labels.append(adjusted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: ['How', 'help', 'survivors', 'massive', 'earthquake', 'Ecuador', 'Disaster', 'Relief', 'Donate']\n",
      "Original labels: [3, 3, 3, 3, 3, 4, 3, 3, 3]\n",
      "\n",
      "Tokenized input: [0, 203, 272, 18016, 3333, 12342, 27641, 23762, 18119, 15362, 2]\n",
      "Adjusted labels: [-100, 3, 3, 3, 3, 3, 4, 3, 3, 3, -100]\n",
      "\n",
      "Length of tokenized input: 11\n",
      "Length of adjusted labels: 11\n",
      "\n",
      "Number of original sentences: 31\n",
      "Number of tokenized sentences: 13157\n",
      "\n",
      "Average original sentence length: 6293.03\n",
      "Average tokenized sentence length: 19.38\n"
     ]
    }
   ],
   "source": [
    "# Print an example to verify\n",
    "print(\"Original sentence:\", processed_train_sentences[2])\n",
    "print(\"Original labels:\", processed_train_labels_ids[2])\n",
    "print(\"\\nTokenized input:\", tokenized_train_inputs[2])\n",
    "print(\"Adjusted labels:\", adjusted_train_labels[2])\n",
    "\n",
    "# Verify lengths\n",
    "print(\"\\nLength of tokenized input:\", len(tokenized_train_inputs[2]))\n",
    "print(\"Length of adjusted labels:\", len(adjusted_train_labels[2]))\n",
    "\n",
    "# Print some statistics\n",
    "original_sentence_count = len(input_ids)\n",
    "tokenized_sentence_count = len(tokenized_train_inputs)\n",
    "print(f\"\\nNumber of original sentences: {original_sentence_count}\")\n",
    "print(f\"Number of tokenized sentences: {tokenized_sentence_count}\")\n",
    "\n",
    "average_original_length = sum(len(s) for s in processed_train_sentences) / original_sentence_count\n",
    "average_tokenized_length = sum(len(s) for s in tokenized_train_inputs) / tokenized_sentence_count\n",
    "print(f\"\\nAverage original sentence length: {average_original_length:.2f}\")\n",
    "print(f\"Average tokenized sentence length: {average_tokenized_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datasets\n",
    "tokenized_train = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_train_inputs,\n",
    "    \"labels\": adjusted_train_labels\n",
    "})\n",
    "tokenized_dev = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_dev_inputs,\n",
    "    \"labels\": adjusted_dev_labels\n",
    "})\n",
    "\n",
    "# Set up label mapping\n",
    "all_labels = set()\n",
    "\n",
    "for labels in train_labels + dev_labels:\n",
    "    all_labels.update(labels)\n",
    "\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "labels = sorted(list(all_labels))\n",
    "\n",
    "label_list = sorted(list(all_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at crisistransformers/CT-M3-Complete and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"crisistransformers/CT-M3-Complete\"\n",
    "\n",
    "# Update model configuration\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = len(label_list)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "\n",
    "CT_M3_Complete_model = AutoModelForTokenClassification.from_pretrained(model_name, config=config)\n",
    "CT_M3_Complete_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = precision_recall_fscore_support(sum(true_labels, []), sum(true_predictions, []), average='weighted')\n",
    "    return {\n",
    "        \"precision\": results[0],\n",
    "        \"recall\": results[1],\n",
    "        \"f1\": results[2],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model and tokenizer\n",
    "model_name = \"crisistransformers/CT-M3-Complete\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = len(label_list)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abelm\\anaconda3\\envs\\mllmrc-gpu\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=CT_M3_Complete_tokenizer)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"lewa/model/CrisisTransformers\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Reduced batch size\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"adamw_torch\",  # Use PyTorch's AdamW implementation\n",
    "    logging_steps=100,  # Reduce logging frequency\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "    report_to='none',  # Disable logging to wandb\n",
    ")\n",
    "\n",
    "# Set up trainer\n",
    "trainer = Trainer(\n",
    "    model=CT_M3_Complete_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    tokenizer=CT_M3_Complete_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6165' max='6165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6165/6165 1:20:22, Epoch 14/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.196100</td>\n",
       "      <td>0.181686</td>\n",
       "      <td>0.940108</td>\n",
       "      <td>0.939146</td>\n",
       "      <td>0.938725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.154143</td>\n",
       "      <td>0.949596</td>\n",
       "      <td>0.948236</td>\n",
       "      <td>0.948812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.152791</td>\n",
       "      <td>0.953723</td>\n",
       "      <td>0.952047</td>\n",
       "      <td>0.952542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.159348</td>\n",
       "      <td>0.953814</td>\n",
       "      <td>0.952816</td>\n",
       "      <td>0.953073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.065500</td>\n",
       "      <td>0.168545</td>\n",
       "      <td>0.954484</td>\n",
       "      <td>0.952530</td>\n",
       "      <td>0.953361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.178426</td>\n",
       "      <td>0.954223</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.953052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.182972</td>\n",
       "      <td>0.954489</td>\n",
       "      <td>0.953192</td>\n",
       "      <td>0.953717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.186439</td>\n",
       "      <td>0.954490</td>\n",
       "      <td>0.952977</td>\n",
       "      <td>0.953588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6165, training_loss=0.0957329041959789, metrics={'train_runtime': 4824.1224, 'train_samples_per_second': 40.91, 'train_steps_per_second': 1.278, 'total_flos': 3819196567202640.0, 'train_loss': 0.0957329041959789, 'epoch': 14.981773997569867})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='206' max='206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [206/206 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18643851578235626, 'eval_precision': 0.9544904084231253, 'eval_recall': 0.9529773833381048, 'eval_f1': 0.953587626043348, 'eval_runtime': 23.7967, 'eval_samples_per_second': 138.212, 'eval_steps_per_second': 8.657, 'epoch': 14.981773997569867}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and associated files saved to /lewa/model/working/results\n"
     ]
    }
   ],
   "source": [
    "# After training\n",
    "output_dir = \"/lewa/model/working/results\"\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "CT_M3_Complete_tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save training arguments\n",
    "with open(f\"{output_dir}/training_args.json\", 'w') as f:\n",
    "    json.dump(training_args.to_dict(), f)\n",
    "\n",
    "# Save label mappings\n",
    "with open(f\"{output_dir}/label_mappings.json\", 'w') as f:\n",
    "    json.dump({\"label2id\": label2id, \"id2label\": id2label}, f)\n",
    "\n",
    "print(f\"Model and associated files saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'B-LOC', '1': 'E-LOC', '2': 'I-LOC', '3': 'O', '4': 'S-LOC'}\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"/lewa/model/working/results\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/lewa/model/working/results\")\n",
    "\n",
    "# Load label mappings\n",
    "with open(\"/lewa/model/working/results/label_mappings.json\", 'r') as f:\n",
    "    label_mappings = json.load(f)\n",
    "\n",
    "id2label = label_mappings[\"id2label\"]\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_subwords_and_locations(tokens_and_labels):\n",
    "    merged_words = []\n",
    "    merged_labels = []\n",
    "    current_word = []\n",
    "    current_labels = []\n",
    "    location_buffer = []\n",
    "\n",
    "    for token, label in tokens_and_labels:\n",
    "        if token.endswith('@@'):\n",
    "            current_word.append(token[:-2])  # Remove '@@'\n",
    "            current_labels.append(label)\n",
    "        else:\n",
    "            current_word.append(token)\n",
    "            current_labels.append(label)\n",
    "\n",
    "            # Merge subwords\n",
    "            merged_word = ''.join(current_word)\n",
    "\n",
    "            # Voting for the label\n",
    "            if len(set(current_labels)) == 1:\n",
    "                merged_label = current_labels[0]\n",
    "            else:\n",
    "                priority_order = ['B-LOC', 'I-LOC', 'E-LOC', 'S-LOC', 'O']\n",
    "                merged_label = next(label for label in priority_order if label in current_labels)\n",
    "\n",
    "            # Handle location merging\n",
    "            if merged_label.endswith('-LOC'):\n",
    "                if merged_label == 'B-LOC' or merged_label == 'S-LOC':\n",
    "                    if location_buffer:\n",
    "                        merged_words.append(' '.join(location_buffer))\n",
    "                        merged_labels.append('B-LOC')\n",
    "                        location_buffer = []\n",
    "                    location_buffer.append(merged_word)\n",
    "                elif merged_label == 'I-LOC' or merged_label == 'E-LOC':\n",
    "                    location_buffer.append(merged_word)\n",
    "                    if merged_label == 'E-LOC':\n",
    "                        merged_words.append(' '.join(location_buffer))\n",
    "                        merged_labels.append('B-LOC')\n",
    "                        location_buffer = []\n",
    "            else:\n",
    "                if location_buffer:\n",
    "                    merged_words.append(' '.join(location_buffer))\n",
    "                    merged_labels.append('B-LOC')\n",
    "                    location_buffer = []\n",
    "                merged_words.append(merged_word)\n",
    "                merged_labels.append(merged_label)\n",
    "\n",
    "            # Reset for next word\n",
    "            current_word = []\n",
    "            current_labels = []\n",
    "\n",
    "    # Handle any remaining location in the buffer\n",
    "    if location_buffer:\n",
    "        merged_words.append(' '.join(location_buffer))\n",
    "        merged_labels.append('B-LOC')\n",
    "\n",
    "    return list(zip(merged_words, merged_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
    "\n",
    "    tokens = []\n",
    "    predicted_tokens = []\n",
    "\n",
    "    locations = []\n",
    "    current_location = []\n",
    "\n",
    "    for token, prediction in zip(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]), predictions[0]):\n",
    "        if int(prediction) == 0:  # Beginning of a new location\n",
    "            current_location = [token]\n",
    "        elif int(prediction) == 2:  # Inside a location\n",
    "            if current_location:  # Make sure we started a location\n",
    "                current_location.append(token)\n",
    "        elif int(prediction) == 1:  # End of a location\n",
    "            if current_location:  # Make sure we're inside a location\n",
    "                current_location.append(token)\n",
    "                locations.append(\" \".join(current_location))\n",
    "                current_location = []\n",
    "        elif int(prediction) == 4:  # Single token location\n",
    "            locations.append(token)\n",
    "        else:\n",
    "            current_location = []  # Reset if prediction is 'O' or anything else\n",
    "\n",
    "        # Remove special tokens and clean up the text\n",
    "        if token not in ['<s>', '</s>', '<unk>']:\n",
    "            cleaned_token = token[1:] if token.startswith('Ġ') else token\n",
    "\n",
    "            if token.startswith('##'):\n",
    "                if predicted_tokens:\n",
    "                    predicted_tokens[-1] = (predicted_tokens[-1][0] + cleaned_token, predicted_tokens[-1][1])\n",
    "                continue\n",
    "\n",
    "            tokens.append(cleaned_token)\n",
    "            predicted_tokens.append((cleaned_token, id2label[str(prediction.item())]))\n",
    "\n",
    "    # Usage\n",
    "    merged_result = merge_subwords_and_locations(predicted_tokens)\n",
    "\n",
    "    # Extract location\n",
    "    locations = []\n",
    "    for word, label in merged_result:\n",
    "     if label.startswith('B-LOC') or label.startswith('S-LOC'):\n",
    "        locations.append(word)\n",
    "\n",
    "    # Extract unique locations and sort alphabetically\n",
    "    unique_locations = sorted(set(locations))\n",
    "\n",
    "    return unique_locations, tokens, predictions, predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abelm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abelm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abelm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"lewa/Test.csv\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.data.path.append('/usr/share/nltk_data/')\n",
    "\n",
    "# Apply preprocessing to each text in your dataset\n",
    "test['processed_text'] = test['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n",
      "Processing row 500\n",
      "Processing row 600\n",
      "Processing row 700\n",
      "Processing row 800\n",
      "Processing row 900\n",
      "Processing row 1000\n",
      "Processing row 1100\n",
      "Processing row 1200\n",
      "Processing row 1300\n",
      "Processing row 1400\n",
      "Processing row 1500\n",
      "Processing row 1600\n",
      "Processing row 1700\n",
      "Processing row 1800\n",
      "Processing row 1900\n",
      "Processing row 2000\n",
      "Processing row 2100\n",
      "Processing row 2200\n",
      "Processing row 2300\n",
      "Processing row 2400\n",
      "Processing row 2500\n",
      "Processing row 2600\n",
      "Processing row 2700\n",
      "Processing row 2800\n",
      "Processing row 2900\n"
     ]
    }
   ],
   "source": [
    "submission = []\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    if index % 100 == 0:\n",
    "        print(f\"Processing row {index}\")\n",
    "\n",
    "    id = row['tweet_id']\n",
    "    processed_text = row['processed_text']\n",
    "\n",
    "    unique_locations, tokens, predictions, predicted_tokens = predict(processed_text)\n",
    "\n",
    "    # Join locations with space, or use a single space if no locations\n",
    "    locations_string = ' '.join(unique_locations) if unique_locations else ' '\n",
    "\n",
    "    submission.append({'ID': id, 'Locations': locations_string})\n",
    "\n",
    "# Create DataFrame from submission list\n",
    "submission_df = pd.DataFrame(submission)\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('lewa/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1001154804658286592</td>\n",
       "      <td>Maryland New England Orleans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1001155505459486720</td>\n",
       "      <td>ELLICOTT CITY MARYLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1001155756371136512</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1001159445194399744</td>\n",
       "      <td>Ellicott City Maryland Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1001164907587538944</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID                     Locations\n",
       "0  ID_1001154804658286592  Maryland New England Orleans\n",
       "1  ID_1001155505459486720        ELLICOTT CITY MARYLAND\n",
       "2  ID_1001155756371136512        Ellicott City Maryland\n",
       "3  ID_1001159445194399744   Ellicott City Maryland Town\n",
       "4  ID_1001164907587538944        Ellicott City Maryland"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
