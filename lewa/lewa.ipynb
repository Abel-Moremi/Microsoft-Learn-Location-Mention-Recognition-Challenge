{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "import nltk\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from datasets import Dataset\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuda and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor is on: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check whether CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.rand(3, 3).to(device)\n",
    "print(f'Tensor is on: {x.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# print cuda version\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lamem\\OneDrive\\Documents\\GHD\\Microsoft-Learn-Location-Mention-Recognition-Challenge\n"
     ]
    }
   ],
   "source": [
    "# check the current working directory\n",
    "os.chdir('C:/Users/lamem/OneDrive/Documents/GHD/Microsoft-Learn-Location-Mention-Recognition-Challenge')\n",
    "#os.chdir('C:/Users/abelm/OneDrive/Documents/GitHub/Microsoft-Learn-Location-Mention-Recognition-Challenge')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1001136212718088192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EllicottCity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1001136696589631488</td>\n",
       "      <td>Flash floods struck a Maryland city on Sunday,...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1001136950345109504</td>\n",
       "      <td>State of emergency declared for Maryland flood...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1001137334056833024</td>\n",
       "      <td>Other parts of Maryland also saw significant d...</td>\n",
       "      <td>Baltimore Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1001138374923579392</td>\n",
       "      <td>Catastrophic Flooding Slams Ellicott City, Mar...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                                               text  \\\n",
       "0  ID_1001136212718088192                                                NaN   \n",
       "1  ID_1001136696589631488  Flash floods struck a Maryland city on Sunday,...   \n",
       "2  ID_1001136950345109504  State of emergency declared for Maryland flood...   \n",
       "3  ID_1001137334056833024  Other parts of Maryland also saw significant d...   \n",
       "4  ID_1001138374923579392  Catastrophic Flooding Slams Ellicott City, Mar...   \n",
       "\n",
       "                 location  \n",
       "0            EllicottCity  \n",
       "1                Maryland  \n",
       "2                Maryland  \n",
       "3      Baltimore Maryland  \n",
       "4  Ellicott City Maryland  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = pd.read_csv(\"lewa/Train_1.csv\")\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# text clean helper function\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # remove mentions only\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove special characters, numbers, and punctuations, keeping spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = text.split()  # Split by spaces without tokenizing\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join words back into a single string\n",
    "    return ' '.join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1001136696589631488</td>\n",
       "      <td>Flash floods struck Maryland city Sunday washi...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1001136950345109504</td>\n",
       "      <td>State emergency declared Maryland flooding via</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1001137334056833024</td>\n",
       "      <td>Other parts Maryland also saw significant dama...</td>\n",
       "      <td>Baltimore Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1001138374923579392</td>\n",
       "      <td>Catastrophic Flooding Slams Ellicott City Mary...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID_1001138377717157888</td>\n",
       "      <td>WATCH missing flash FLOODING devastates Ellico...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                                               text  \\\n",
       "1  ID_1001136696589631488  Flash floods struck Maryland city Sunday washi...   \n",
       "2  ID_1001136950345109504     State emergency declared Maryland flooding via   \n",
       "3  ID_1001137334056833024  Other parts Maryland also saw significant dama...   \n",
       "4  ID_1001138374923579392  Catastrophic Flooding Slams Ellicott City Mary...   \n",
       "5  ID_1001138377717157888  WATCH missing flash FLOODING devastates Ellico...   \n",
       "\n",
       "                 location  \n",
       "1                Maryland  \n",
       "2                Maryland  \n",
       "3      Baltimore Maryland  \n",
       "4  Ellicott City Maryland  \n",
       "5  Ellicott City Maryland  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your Â \n",
    "df_t = new.copy() \n",
    "df_t = df_t.dropna(subset = \"text\")\n",
    "# clean the text\n",
    "df_t['text'] = df_t['text'].apply(clean_text)\n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16448, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into train and dev\n",
    "trainData, devData = train_test_split(df_t, test_size=0.15, random_state=42)\n",
    "\n",
    "# save data to a csv tainData\n",
    "trainData.to_csv('lewa/Train-dropna.csv', index=False)\n",
    "\n",
    "# save data to a csv devData\n",
    "devData.to_csv('lewa/Dev-dropna.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def csv_to_json(csv_file_path, json_file_path):\n",
    "    # Open the CSV file and create a list of dictionaries\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        data = [row for row in csv_reader]\n",
    "\n",
    "    # Write the list of dictionaries to a JSON file\n",
    "    with open(json_file_path, mode='w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "#csv_file_path = 'testData/Train-dropna.csv'  # Path to your CSV file\n",
    "#json_file_path = 'testData/output_file.json'  # Path where the JSON will be saved\n",
    "csv_to_json('lewa/Train-dropna.csv', 'lewa/train_file.json')\n",
    "csv_to_json('lewa/Dev-dropna.csv', 'lewa/dev_file.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDENTIFY location from sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def remove_hashtag(text):\n",
    "    # Find hashtags and process each one\n",
    "    return re.sub(r'#(\\w+)', lambda m: ' '.join(re.findall(r'[A-Z][^A-Z]*|[a-z]+', m.group(1))), text)\n",
    "\n",
    "# Function to clean and preprocess text (tokenization, normalization, lemmatization)\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters (keeping alphanumeric characters and spaces)\n",
    "    text = remove_hashtag(text)  # Remove hashtags and separate words\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    cleaned_tokens = []\n",
    "    for token in doc:\n",
    "        # Normalize: Convert to lowercase and remove punctuation\n",
    "        if token.text not in string.punctuation:\n",
    "            cleaned_tokens.append(token.lemma_.lower())  # Lemmatize and lowercase\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# Function to extract locations as entities after preprocessing\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'GPE':  # 'GPE' is for locations\n",
    "            entities.append({\n",
    "                'entity': ent.text,\n",
    "                'label': ent.label_,\n",
    "                'start_char': ent.start_char,\n",
    "                'end_char': ent.end_char\n",
    "            })\n",
    "    return entities\n",
    "\n",
    "# Function to process a JSON file\n",
    "def process_json_file(file_path):\n",
    "    # Load JSON data\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert the JSON data to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Preprocess text and extract entities\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    df['entities'] = df['cleaned_text'].apply(extract_entities)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Paths to your two JSON files\n",
    "file_path_1 = 'lewa/train_file.json'\n",
    "file_path_2 = 'lewa/dev_file.json'\n",
    "\n",
    "# Process both files\n",
    "df1 = process_json_file(file_path_1)\n",
    "df2 = process_json_file(file_path_2)\n",
    "\n",
    "\n",
    "# If you want to save them separately, you can use the individual DataFrames:\n",
    "# Save the updated data for file1\n",
    "with open('lewa/train_entities.json', 'w') as f:\n",
    "    json.dump(df1.to_dict(orient='records'), f, indent=4)\n",
    "\n",
    "# Save the updated data for file2\n",
    "with open('lewa/dev_entities.json', 'w') as f:\n",
    "    json.dump(df2.to_dict(orient='records'), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Merge the extracted_locations with the Existing location Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to merge location and entities, taking file paths for input and output\n",
    "def merge_entities(input_file_path, output_file_path):\n",
    "    # Read the unmerged JSON file\n",
    "    with open(input_file_path, 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "    \n",
    "    for entry in data:\n",
    "        location = entry.get(\"location\")\n",
    "        entities = entry.get(\"entities\", [])\n",
    "        \n",
    "        # Create the merged_entities field with only entities present in the entities list\n",
    "        entry[\"merged_entities\"] = [ent[\"entity\"] for ent in entities]\n",
    "\n",
    "        merged_entities = [ent[\"entity\"] for ent in entities]\n",
    "\n",
    "        # If location is present and valid, merge it with entities\n",
    "        if location:\n",
    "            merged_entities.append(location)\n",
    "\n",
    "        # Update the entry with merged entities\n",
    "        entry[\"merged_entities\"] = merged_entities\n",
    "    \n",
    "    # Write the updated data to the output JSON file\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your two JSON files\n",
    "input_file_path = 'lewa/train_entities.json'\n",
    "output_file_path = 'lewa/train_merged_entities.json'\n",
    "\n",
    "# Merge entities for the first file\n",
    "merge_entities(input_file_path, output_file_path)\n",
    "\n",
    "# Paths to your two JSON files\n",
    "input_file_path = 'lewa/dev_entities.json'\n",
    "output_file_path = 'lewa/dev_merged_entities.json'\n",
    "\n",
    "# Merge entities for the first file\n",
    "merge_entities(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIOES TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to read JSON file and return a DataFrame with specific columns\n",
    "def json_to_csv(input_file_path, output_csv_path):\n",
    "    # Read the JSON file\n",
    "    with open(input_file_path, 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "    \n",
    "    # Create a list to hold the data for the DataFrame\n",
    "    rows = []\n",
    "    \n",
    "    for entry in data:\n",
    "        tweet_id = entry.get(\"tweet_id\")\n",
    "        cleaned_text = entry.get(\"cleaned_text\")\n",
    "        merged_entities = entry.get(\"merged_entities\", [])\n",
    "        \n",
    "        # Convert the list of entities into a single string (join them with a comma and space)\n",
    "        location = ' '.join(merged_entities) if merged_entities else None\n",
    "        \n",
    "        # Add the row to the list\n",
    "        rows.append({\n",
    "            \"tweet_id\": tweet_id,\n",
    "            \"text\": cleaned_text,  # Rename cleaned_text to text\n",
    "            \"location\": location  # Convert merged_entities to a plain text string\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame from the list of rows\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save merged_train_entities to a csv\n",
    "train_file_path = 'lewa/train_merged_entities.json'\n",
    "train_csv_path = 'lewa/train_merged_entities.csv'\n",
    "json_to_csv(train_file_path, train_csv_path)\n",
    "\n",
    "# save merged_dev_entities to a csv\n",
    "dev_file_path = 'lewa/dev_merged_entities.json' \n",
    "dev_csv_path = 'lewa/dev_merged_entities.csv'\n",
    "json_to_csv(dev_file_path, dev_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bioes_tagging(text, merged_entities):\n",
    "    \"\"\"\n",
    "    Apply BIOES tagging to a tokenized text based on merged entity spans.\n",
    "    \n",
    "    :param text: List of tokenized words.\n",
    "    :param merged_entities: List of merged location entities in the sentence (already a list).\n",
    "    :return: List of tuples (word, tag) in BIOES format.\n",
    "    \"\"\"\n",
    "    tags = ['O'] * len(text)\n",
    "    \n",
    "    # Process each merged entity (location)\n",
    "    for entity in merged_entities:\n",
    "        entity_tokens = entity.split()  # Assuming each entity is a string of tokens\n",
    "        entity_length = len(entity_tokens)\n",
    "\n",
    "        # Apply BIOES tags for each token in the entity\n",
    "        for i, token in enumerate(entity_tokens):\n",
    "            try:\n",
    "                token_index = text.index(token)\n",
    "            except ValueError:\n",
    "                continue  # Skip tokens not found in the sentence\n",
    "\n",
    "            if entity_length == 1:\n",
    "                tags[token_index] = 'S-LOCATION'\n",
    "            elif i == 0:\n",
    "                tags[token_index] = 'B-LOCATION'\n",
    "            elif i == entity_length - 1:\n",
    "                tags[token_index] = 'E-LOCATION'\n",
    "            else:\n",
    "                tags[token_index] = 'I-LOCATION'\n",
    "    \n",
    "    return list(zip(text, tags))\n",
    "\n",
    "def process_json(input_file, output_file):\n",
    "    # Read the JSON file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    output_data = []\n",
    "\n",
    "    # Process each entry in the JSON file\n",
    "    for entry in data:\n",
    "        text = entry['text']  # Assuming 'text' field has the sentence\n",
    "        merged_entities = entry.get('merged_entities', [])  # No split needed, it's already a list\n",
    "        \n",
    "        # Tokenize the text (basic tokenization)\n",
    "        tokens = text.split()\n",
    "\n",
    "        # Apply BIOES tagging\n",
    "        tagged_tokens = bioes_tagging(tokens, merged_entities)\n",
    "        \n",
    "        # Prepare output format\n",
    "        for token, tag in tagged_tokens:\n",
    "            output_data.append(f\"{token}\\t{tag}\")\n",
    "        output_data.append(\"\")  # New line after each sentence\n",
    "\n",
    "    # Write the output to a text file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(output_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "process_json('lewa/train_merged_entities.json', 'lewa/train_bioes.txt')\n",
    "process_json('lewa/dev_merged_entities.json', 'lewa/dev_bioes.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
