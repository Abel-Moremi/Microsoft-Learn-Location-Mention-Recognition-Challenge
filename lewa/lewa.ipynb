{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lamem\\anaconda3\\Anaconda3\\envs\\mllmrc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "import nltk\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from datasets import Dataset\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuda and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.rand(3, 3).to(device)\n",
    "print(f'Tensor is on: {x.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print cuda version\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lamem\\OneDrive\\Documents\\GHD\\Microsoft-Learn-Location-Mention-Recognition-Challenge\n"
     ]
    }
   ],
   "source": [
    "# check the current working directory\n",
    "os.chdir('C:/Users/lamem/OneDrive/Documents/GHD/Microsoft-Learn-Location-Mention-Recognition-Challenge')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1001136212718088192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EllicottCity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1001136696589631488</td>\n",
       "      <td>Flash floods struck a Maryland city on Sunday,...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1001136950345109504</td>\n",
       "      <td>State of emergency declared for Maryland flood...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1001137334056833024</td>\n",
       "      <td>Other parts of Maryland also saw significant d...</td>\n",
       "      <td>Baltimore Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1001138374923579392</td>\n",
       "      <td>Catastrophic Flooding Slams Ellicott City, Mar...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                                               text  \\\n",
       "0  ID_1001136212718088192                                                NaN   \n",
       "1  ID_1001136696589631488  Flash floods struck a Maryland city on Sunday,...   \n",
       "2  ID_1001136950345109504  State of emergency declared for Maryland flood...   \n",
       "3  ID_1001137334056833024  Other parts of Maryland also saw significant d...   \n",
       "4  ID_1001138374923579392  Catastrophic Flooding Slams Ellicott City, Mar...   \n",
       "\n",
       "                 location  \n",
       "0            EllicottCity  \n",
       "1                Maryland  \n",
       "2                Maryland  \n",
       "3      Baltimore Maryland  \n",
       "4  Ellicott City Maryland  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = pd.read_csv(\"lewa/Train_1.csv\")\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_1001136696589631488</td>\n",
       "      <td>Flash floods struck a Maryland city on Sunday,...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_1001136950345109504</td>\n",
       "      <td>State of emergency declared for Maryland flood...</td>\n",
       "      <td>Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_1001137334056833024</td>\n",
       "      <td>Other parts of Maryland also saw significant d...</td>\n",
       "      <td>Baltimore Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_1001138374923579392</td>\n",
       "      <td>Catastrophic Flooding Slams Ellicott City, Mar...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID_1001138377717157888</td>\n",
       "      <td>WATCH: 1 missing after flash #FLOODING devasta...</td>\n",
       "      <td>Ellicott City Maryland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                                               text  \\\n",
       "1  ID_1001136696589631488  Flash floods struck a Maryland city on Sunday,...   \n",
       "2  ID_1001136950345109504  State of emergency declared for Maryland flood...   \n",
       "3  ID_1001137334056833024  Other parts of Maryland also saw significant d...   \n",
       "4  ID_1001138374923579392  Catastrophic Flooding Slams Ellicott City, Mar...   \n",
       "5  ID_1001138377717157888  WATCH: 1 missing after flash #FLOODING devasta...   \n",
       "\n",
       "                 location  \n",
       "1                Maryland  \n",
       "2                Maryland  \n",
       "3      Baltimore Maryland  \n",
       "4  Ellicott City Maryland  \n",
       "5  Ellicott City Maryland  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your Â \n",
    "df_t = new.copy() \n",
    "df_t = df_t.dropna(subset = \"text\")\n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16448, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into train and dev\n",
    "trainData, devData = train_test_split(df_t, test_size=0.15, random_state=42)\n",
    "\n",
    "# save data to a csv tainData\n",
    "trainData.to_csv('lewa/Train-dropna.csv', index=False)\n",
    "\n",
    "# save data to a csv devData\n",
    "devData.to_csv('lewa/Dev-dropna.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def csv_to_json(csv_file_path, json_file_path):\n",
    "    # Open the CSV file and create a list of dictionaries\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        data = [row for row in csv_reader]\n",
    "\n",
    "    # Write the list of dictionaries to a JSON file\n",
    "    with open(json_file_path, mode='w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "# Example usage\n",
    "#csv_file_path = 'testData/Train-dropna.csv'  # Path to your CSV file\n",
    "#json_file_path = 'testData/output_file.json'  # Path where the JSON will be saved\n",
    "csv_to_json('lewa/Train-dropna.csv', 'lewa/train_file.json')\n",
    "csv_to_json('lewa/Dev-dropna.csv', 'lewa/dev_file.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDENTIFY location from sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def remove_hashtag(text):\n",
    "    # Find hashtags and process each one\n",
    "    return re.sub(r'#(\\w+)', lambda m: ' '.join(re.findall(r'[A-Z][^A-Z]*|[a-z]+', m.group(1))), text)\n",
    "\n",
    "# Function to clean and preprocess text (tokenization, normalization, lemmatization)\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters (keeping alphanumeric characters and spaces)\n",
    "    text = remove_hashtag(text)  # Remove hashtags and separate words\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    cleaned_tokens = []\n",
    "    for token in doc:\n",
    "        # Normalize: Convert to lowercase and remove punctuation\n",
    "        if token.text not in string.punctuation:\n",
    "            cleaned_tokens.append(token.lemma_)  # Lemmatize and lowercase\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# Function to extract locations as entities after preprocessing\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'GPE':  # 'GPE' is for locations\n",
    "            entities.append({\n",
    "                'entity': ent.text,\n",
    "                'label': ent.label_,\n",
    "                'start_char': ent.start_char,\n",
    "                'end_char': ent.end_char\n",
    "            })\n",
    "    return entities\n",
    "\n",
    "# Function to process a JSON file\n",
    "def process_json_file(file_path):\n",
    "    # Load JSON data\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert the JSON data to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Preprocess text and extract entities\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    df['entities'] = df['cleaned_text'].apply(extract_entities)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Paths to your two JSON files\n",
    "file_path_1 = 'lewa/train_file.json'\n",
    "file_path_2 = 'lewa/dev_file.json'\n",
    "\n",
    "# Process both files\n",
    "df1 = process_json_file(file_path_1)\n",
    "df2 = process_json_file(file_path_2)\n",
    "\n",
    "\n",
    "# If you want to save them separately, you can use the individual DataFrames:\n",
    "# Save the updated data for file1\n",
    "with open('lewa/train_entities.json', 'w') as f:\n",
    "    json.dump(df1.to_dict(orient='records'), f, indent=4)\n",
    "\n",
    "# Save the updated data for file2\n",
    "with open('lewa/dev_entities.json', 'w') as f:\n",
    "    json.dump(df2.to_dict(orient='records'), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Merge the extracted_locations with the Existing location Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize text case\n",
    "\n",
    "def merge_locations(df, existing_column='location', extracted_column='entities'):\n",
    "    # Ensure that the existing location column is always a list\n",
    "    df[existing_column] = df[existing_column].apply(lambda x: [x] if isinstance(x, str) else [])\n",
    "\n",
    "    # Ensure extracted locations are lists of strings\n",
    "    df[extracted_column] = df[extracted_column].apply(\n",
    "        lambda locs: [loc for loc in locs if isinstance(loc, str)] if isinstance(locs, list) else []\n",
    "    )\n",
    "\n",
    "    # Combine existing and extracted locations, ensuring all entries are strings\n",
    "    df['merged_locations'] = df.apply(\n",
    "        lambda row: list(set(row[existing_column] + row[extracted_column])), axis=1\n",
    "    )\n",
    "\n",
    "    # Optionally, drop the intermediate columns if you don't need them\n",
    "    #df.drop([existing_column, extracted_column], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "file_path_1 = 'lewa/train_entities.json'\n",
    "file_path_2 = 'lewa/dev_entities.json'\n",
    "\n",
    "# Process both files\n",
    "df1 = process_json_file(file_path_1)\n",
    "df2 = process_json_file(file_path_2)\n",
    "\n",
    "df1 = merge_locations(df1)\n",
    "df2 = merge_locations(df2)\n",
    "\n",
    "# Remove duplicates within each list (if needed)\n",
    "df1['merged_locations'] = df1['merged_locations'].apply(lambda locs: list(set(locs)))\n",
    "df2['merged_locations'] = df2['merged_locations'].apply(lambda locs: list(set(locs)))\n",
    "\n",
    "# Save the merged data for file1\n",
    "with open('lewa/train_merged_entities.json', 'w') as f:\n",
    "    json.dump(df1.to_dict(orient='records'), f, indent=4)\n",
    "\n",
    "# Save the merged data for file2\n",
    "with open('lewa/dev_merged_entities.json', 'w') as f:\n",
    "    json.dump(df2.to_dict(orient='records'), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIOES TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
